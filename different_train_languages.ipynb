{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "different_train_languages.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/work_in_progress/blob/master/different_train_languages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LdkIU9sMbRB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "b4a3c262-c925-46b6-9632-fad65079c7cb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIBRYlYK_6tP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from kaggle_datasets import KaggleDatasets\n",
        "import transformers\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "from tqdm.notebook import tqdm\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiPSp8uEAAQU",
        "colab_type": "text"
      },
      "source": [
        "methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NO6hCI4__2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def regular_encode(texts, tokenizer, maxlen=512):\n",
        "    enc_di = tokenizer.batch_encode_plus(\n",
        "        texts, \n",
        "        return_attention_masks=False, \n",
        "        return_token_type_ids=False,\n",
        "        pad_to_max_length=True,\n",
        "        max_length=maxlen\n",
        "    )\n",
        "    \n",
        "    return np.array(enc_di['input_ids'])\n",
        "def build_model(transformer, max_len=512):\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    sequence_output = transformer(input_word_ids)[0]\n",
        "    cls_token = sequence_output[:, 0, :]\n",
        "    out = Dense(1, activation='sigmoid')(cls_token)\n",
        "    \n",
        "    model = Model(inputs=input_word_ids, outputs=out)\n",
        "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj2fD9PjAGno",
        "colab_type": "text"
      },
      "source": [
        "setting tpu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q3mCAVGAGx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Detect hardware, return appropriate distribution strategy\n",
        "try:\n",
        "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
        "    # set: this is always the case on Kaggle.\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKLM055X_dxy",
        "colab_type": "text"
      },
      "source": [
        "setting parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NNE0rsZ_dFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "\n",
        "# Configuration\n",
        "EPOCHS = 2\n",
        "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "MAX_LEN = 192\n",
        "MODEL = 'jplu/tf-xlm-roberta-large'\n",
        "lang=\"es\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma888SwqARsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First load the real tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsiPqHMGNV6-",
        "colab_type": "text"
      },
      "source": [
        "downloading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DgxaKpOMd9c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "969e2b55-8d51-4bdc-dff3-9ec760ac216d"
      },
      "source": [
        "from zipfile import ZipFile \n",
        "path = F\"/content/gdrive/My Drive/multilingual toxic/\" \n",
        "with ZipFile(path+'toxic.zip', 'r') as zip: \n",
        "    # printing all the contents of the zip file \n",
        "    zip.printdir() \n",
        "  \n",
        "    # extracting all the files \n",
        "    print('Extracting all the files now...') \n",
        "    zip.extractall() \n",
        "    print('Done!')\n",
        "with ZipFile(path+'different_train_lang.zip.zip', 'r') as zip: \n",
        "    # printing all the contents of the zip file \n",
        "    zip.printdir() \n",
        "  \n",
        "    # extracting all the files \n",
        "    print('Extracting all the files now...') \n",
        "    zip.extractall() \n",
        "    print('Done!')\n",
        "import pandas as pd\n",
        "def load_data():\n",
        "    trn=pd.read_csv('train_'+lang+'_cleaned.csv',usecols=['toxic','comment_text'])\n",
        "    val=pd.read_csv('validation.csv',usecols=['toxic','comment_text','lang'])\n",
        "    tst=pd.read_csv('test.csv',usecols=['lang','content'])    \n",
        "    return trn,tst,val\n",
        "train,test,valid=load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0MkL3H0Nk1z",
        "colab_type": "text"
      },
      "source": [
        "Encoding data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L62Y_Ub5NjNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time \n",
        "x_train = regular_encode(train.comment_text.values, tokenizer, maxlen=192)\n",
        "x_valid = regular_encode(valid.comment_text.values, tokenizer, maxlen=192)\n",
        "x_test = regular_encode(test.content.values, tokenizer, maxlen=192)\n",
        "\n",
        "y_train = train.toxic.values\n",
        "y_valid = valid.toxic.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLK9VtUiAZtA",
        "colab_type": "text"
      },
      "source": [
        "Preparing datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Anpwx9gqAZ3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_train, y_train))\n",
        "    .repeat()\n",
        "    .shuffle(2048)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "valid_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_valid, y_valid))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .cache()\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "test_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices(x_test)\n",
        "    .batch(BATCH_SIZE)\n",
        ")\n",
        "\n",
        "trn_ln=train.shape[0]\n",
        "val_len=valid.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tL8-M9HAoTi",
        "colab_type": "text"
      },
      "source": [
        "Verfying model stricture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls23cnivAgZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "with strategy.scope():\n",
        "    transformer_layer=TFAutoModel.from_pretrained(MODEL)\n",
        "    model=build_model(transformer_layer,192)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z31haAgNowq",
        "colab_type": "text"
      },
      "source": [
        "Fitting training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O85qsxZgNnFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_steps = x_train.shape[0] // BATCH_SIZE\n",
        "train_history = model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=n_steps,\n",
        "    validation_data=valid_dataset,\n",
        "    epochs=EPOCHS\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EFdP_P9Asr9",
        "colab_type": "text"
      },
      "source": [
        "Fitting validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFlKlJ_nAs5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_steps = x_valid.shape[0] // BATCH_SIZE\n",
        "train_history_2 = model.fit(\n",
        "    valid_dataset.repeat(),\n",
        "    steps_per_epoch=n_steps,\n",
        "    epochs=EPOCHS\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o5YZtzYNuar",
        "colab_type": "text"
      },
      "source": [
        "Predicting test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m14d_lXNr6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub['toxic'] = model.predict(test_dataset, verbose=1)\n",
        "sub.to_csv('submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN-d2HJQA3nO",
        "colab_type": "text"
      },
      "source": [
        "Predicting train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC5Qd46pTPIw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e218770a-af34-4ff6-f810-b57dba8853fc"
      },
      "source": [
        "pre = model.predict(valid_dataset, verbose=1)\n",
        "np.save('valid', pre)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 224, 224, 32) 416         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 112, 112, 32) 0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 111, 111, 64) 8256        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 56, 56, 64)   0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 55, 55, 24)   6168        max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 3025, 24)     0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3025, 128)    3072        reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 3025, 128)    3072        reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, None, 64)     0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, None, 64)     0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, None, None)   0           lambda_1[0][0]                   \n",
            "                                                                 lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, None, None)   0           lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 3025, 128)    3072        reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, None, None)   0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, None, 64)     0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, None, 64)     0           dropout_1[0][0]                  \n",
            "                                                                 lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, None, 128)    0           lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, None, 24)     3096        lambda_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, None, 24)     0           time_distributed_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, None, 24)     0           dropout_2[0][0]                  \n",
            "                                                                 reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_1 (LayerNor (None, None, 24)     48          add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 55, 55, 24)   0           layer_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 72600)        0           reshape_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 64)           4646464     flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 3)            195         dense_5[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 4,673,859\n",
            "Trainable params: 4,673,859\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}