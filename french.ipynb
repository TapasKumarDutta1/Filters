{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "french.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO5FqRWT2OQKfP+TVq/nipb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/multilingial/blob/master/french.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThL3La8VosEV",
        "outputId": "d749366c-2fb4-42ab-aa0e-c4de648cef8c"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/40/866cbfac4601e0f74c7303d533a9c5d4a53858bd402e08e3e294dd271f25/transformers-4.2.1-py3-none-any.whl (1.8MB)\n",
            "\r\u001b[K     |▏                               | 10kB 15.5MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 20.9MB/s eta 0:00:01\r\u001b[K     |▋                               | 30kB 11.0MB/s eta 0:00:01\r\u001b[K     |▊                               | 40kB 8.2MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 4.5MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61kB 4.9MB/s eta 0:00:01\r\u001b[K     |█▎                              | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 81kB 5.3MB/s eta 0:00:01\r\u001b[K     |█▊                              | 92kB 5.6MB/s eta 0:00:01\r\u001b[K     |█▉                              | 102kB 5.8MB/s eta 0:00:01\r\u001b[K     |██                              | 112kB 5.8MB/s eta 0:00:01\r\u001b[K     |██▎                             | 122kB 5.8MB/s eta 0:00:01\r\u001b[K     |██▍                             | 133kB 5.8MB/s eta 0:00:01\r\u001b[K     |██▋                             | 143kB 5.8MB/s eta 0:00:01\r\u001b[K     |██▉                             | 153kB 5.8MB/s eta 0:00:01\r\u001b[K     |███                             | 163kB 5.8MB/s eta 0:00:01\r\u001b[K     |███▏                            | 174kB 5.8MB/s eta 0:00:01\r\u001b[K     |███▍                            | 184kB 5.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 194kB 5.8MB/s eta 0:00:01\r\u001b[K     |███▊                            | 204kB 5.8MB/s eta 0:00:01\r\u001b[K     |████                            | 215kB 5.8MB/s eta 0:00:01\r\u001b[K     |████                            | 225kB 5.8MB/s eta 0:00:01\r\u001b[K     |████▎                           | 235kB 5.8MB/s eta 0:00:01\r\u001b[K     |████▌                           | 245kB 5.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 256kB 5.8MB/s eta 0:00:01\r\u001b[K     |████▉                           | 266kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 276kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 286kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 296kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 307kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 317kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 327kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 337kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 348kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 358kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 368kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 378kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 389kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 399kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 409kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 419kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 430kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 440kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 450kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 460kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 471kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 481kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 491kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 501kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 512kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 522kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 532kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 542kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 552kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 563kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 573kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 583kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 593kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 604kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 614kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 624kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 634kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 645kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 655kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 665kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 675kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 686kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 696kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 706kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 716kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 727kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 737kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 747kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 757kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 768kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 778kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 788kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 798kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 808kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 819kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 829kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 839kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 849kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 860kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 870kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 880kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 890kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 901kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 911kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 921kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 931kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 942kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 952kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 962kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 972kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 983kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 993kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.0MB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.0MB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.0MB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.0MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.0MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.1MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.1MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.1MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.1MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.1MB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1MB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.1MB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.1MB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.1MB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.1MB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.2MB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.2MB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.2MB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.2MB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.2MB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.2MB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2MB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.2MB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.2MB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.2MB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.3MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.3MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.3MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.3MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.3MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.3MB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.3MB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.3MB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.3MB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.4MB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.4MB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.4MB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.4MB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.4MB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.4MB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.4MB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.4MB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.4MB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.4MB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.5MB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.5MB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.5MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.5MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.5MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.5MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.5MB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.5MB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.6MB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.6MB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.6MB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.6MB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.6MB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.6MB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.6MB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.6MB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.6MB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.6MB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.7MB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.7MB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.7MB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.7MB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.7MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.7MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.7MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.7MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.7MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.8MB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.8MB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.8MB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.22.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.35)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 47.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "Successfully installed tokenizers-0.9.4 transformers-4.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0BCcBhgtK_T"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel\r\n",
        "\t\t\t\t\t\t\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")\r\n",
        "\t\t\t\t\t\t\r\n",
        "model = AutoModel.from_pretrained(\"camembert-base\")\r\n",
        "\t\t\t\t\t"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4t8SlAXnj4m",
        "outputId": "fb2d9246-f529-4f3c-c94d-fc2cde8d03e5"
      },
      "source": [
        "model"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CamembertModel(\n",
              "  (embeddings): RobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): RobertaEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): RobertaPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTmyxVfLsdZ_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}