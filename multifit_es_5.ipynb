{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multifit_es_5.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/multilingial/blob/master/multifit_es_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Zg7Z1MxwxST",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a0194ad7-46de-492e-af3b-b73be4031a7c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtuVe1YOZSm3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "036dffd7-682c-4b81-c7a0-d52b84283e44"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2idGzXgEV5V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "bbc07af4-7480-40ea-bda8-5d2c6f53592b"
      },
      "source": [
        "pip install git+https://github.com/n-waves/multifit"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/n-waves/multifit\n",
            "  Cloning https://github.com/n-waves/multifit to /tmp/pip-req-build-n51qoblc\n",
            "  Running command git clone -q https://github.com/n-waves/multifit /tmp/pip-req-build-n51qoblc\n",
            "Requirement already satisfied (use --upgrade to upgrade): multifit==1.0 from git+https://github.com/n-waves/multifit in /usr/local/lib/python3.6/dist-packages\n",
            "Building wheels for collected packages: multifit\n",
            "  Building wheel for multifit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for multifit: filename=multifit-1.0-cp36-none-any.whl size=24506 sha256=6757eb70cb06e46b342b530aeb5d5b768bbb79195802c085cb59766a762eab33\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-smomkkl4/wheels/b7/92/1e/246f31a4e84fd665b5907cb96765f696be45e814ff68a5fd4a\n",
            "Successfully built multifit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4YeJJ3DE2YA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "bf62f2e5-7a81-4d0a-a358-cd5d8063ca67"
      },
      "source": [
        "pip install sacremoses"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (0.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sacremoses) (4.41.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMYSglC4GCBl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "395f32b2-b75f-43d6-b7a1-d39d2a12532b"
      },
      "source": [
        "pip install sentencepiece"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.91)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apj6OOR7JtnE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "a77f0985-5ec5-429c-8d7f-233737e7ab99"
      },
      "source": [
        "from zipfile import ZipFile \n",
        "path = F\"/content/gdrive/My Drive/multilingual toxic/\" \n",
        "with ZipFile(path+'toxic.zip', 'r') as zip: \n",
        "    # printing all the contents of the zip file \n",
        "    zip.printdir() \n",
        "  \n",
        "    # extracting all the files \n",
        "    print('Extracting all the files now...') \n",
        "    zip.extractall() \n",
        "    print('Done!')\n",
        "import pandas as pd\n",
        "def load_data():\n",
        "    trn=pd.read_csv('jigsaw-toxic-comment-train.csv',usecols=['toxic','comment_text'])\n",
        "    val=pd.read_csv('validation.csv',usecols=['toxic','comment_text','lang'])\n",
        "    tst=pd.read_csv('test.csv',usecols=['lang','content'])  \n",
        "    sub=pd.read_csv('submission (1).csv')  \n",
        "    return trn,tst,val,sub\n",
        "train,test,valid,sub=load_data()\n",
        "test['toxic']=sub['toxic'].round()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Name                                             Modified             Size\n",
            "jigsaw-toxic-comment-train.csv                 2020-06-08 23:38:40     95538001\n",
            "test.csv                                       2020-06-08 23:39:02     28783206\n",
            "validation.csv                                 2020-06-08 23:38:52      3178555\n",
            "submission (1).csv                             2020-06-08 23:50:22      1161879\n",
            "Extracting all the files now...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OZ0ykdiRV6L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8521b9ff-d6d2-419d-d4d0-947c1c606f2a"
      },
      "source": [
        "print(train.shape,test.shape,valid.shape,sub.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(223549, 2) (63812, 3) (8000, 3) (63812, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvS019LgKbjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import transformers\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "from tqdm.notebook import tqdm\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
        "from tensorflow.keras.layers import *\n",
        "from fastai.text import *\n",
        "import multifit\n",
        "exp = multifit.from_pretrained(\"es_multifit_paper_version\")\n",
        "def regular_encode(texts, tokenizer, maxlen=512):\n",
        "    enc_di = tokenizer.batch_encode_plus(\n",
        "        texts, \n",
        "        return_attention_masks=False, \n",
        "        return_token_type_ids=False,\n",
        "        pad_to_max_length=True,\n",
        "        max_length=maxlen\n",
        "    )\n",
        "    \n",
        "    return np.array(enc_di['input_ids'])\n",
        "fa_config =  exp.pretrain_lm.tokenizer.get_fastai_config(add_open_file_processor=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEikVSPlQkGv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "25b2f4b2-5cb7-417c-c87d-3f6e37fbe712"
      },
      "source": [
        "es=test.loc[test['lang']=='es']\n",
        "es['id']=es.index\n",
        "es=es.reset_index(drop=True)\n",
        "es.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>lang</th>\n",
              "      <th>toxic</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>el skate es unos de los deportes favoritos de ...</td>\n",
              "      <td>es</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Me doy la bienvenida. A este usuari le gusta c...</td>\n",
              "      <td>es</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ES NOTABLEMENTE TENDENCIOSO, NO SE HABLA DE CU...</td>\n",
              "      <td>es</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>El Jardín de infantes Nº938, fundado en 1989, ...</td>\n",
              "      <td>es</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Daré explicaciones y/o aclaraciones a cualquie...</td>\n",
              "      <td>es</td>\n",
              "      <td>0.0</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content lang  toxic  id\n",
              "0  el skate es unos de los deportes favoritos de ...   es    0.0   7\n",
              "1  Me doy la bienvenida. A este usuari le gusta c...   es    0.0   8\n",
              "2  ES NOTABLEMENTE TENDENCIOSO, NO SE HABLA DE CU...   es    0.0   9\n",
              "3  El Jardín de infantes Nº938, fundado en 1989, ...   es    0.0  22\n",
              "4  Daré explicaciones y/o aclaraciones a cualquie...   es    0.0  49"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvuqbUaHZtUq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "4f06cf34-53f9-4a36-b5cf-c742ce61d4d4"
      },
      "source": [
        "# Detect hardware, return appropriate distribution strategy\n",
        "try:\n",
        "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
        "    # set: this is always the case on Kaggle.\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running on TPU  grpc://10.17.82.218:8470\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.17.82.218:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.17.82.218:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "REPLICAS:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeJdr-HQA_BX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d7042c0-2f1a-4b11-ec79-a069ac7cf3b3"
      },
      "source": [
        "es['toxic']=es['toxic'].astype(np.long)\n",
        "ln=es.shape[0]//5\n",
        "trn=es.loc[:4*ln].reset_index(drop=True)\n",
        "val=es.loc[4*ln:].reset_index(drop=True)\n",
        "print(trn.shape,val.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6749, 4) (1690, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4eDzDHjY77y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "255b30f8-5a72-42bc-bcfc-ab63225deef2"
      },
      "source": [
        "with strategy.scope():\n",
        "    exp = multifit.from_pretrained(\"es_multifit_paper_version\")\n",
        "    fa_config =  exp.pretrain_lm.tokenizer.get_fastai_config(add_open_file_processor=True)\n",
        "    data_lm = (TextList.from_df(es, **fa_config)\n",
        "            .split_by_rand_pct(0.1)\n",
        "            .label_for_lm()           \n",
        "            .databunch(bs=32))\n",
        "    learn = exp.finetune_lm.get_learner(data_lm)  \n",
        "    # learn is a preconfigured fastai learner with a pretrained model loaded\n",
        "data_lm.show_batch()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Training args:  {'drop_mult': 0.3, 'true_wd': False, 'wd': 1e-07, 'pretrained': False, 'clip': 0.12} config:  {'emb_sz': 400, 'n_hid': 1550, 'n_layers': 4, 'pad_token': 1, 'qrnn': True, 'bidir': False, 'output_p': 0.25, 'hidden_p': 0.1, 'input_p': 0.2, 'embed_p': 0.02, 'weight_p': 0.15, 'tie_weights': True, 'out_bias': True}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/setuptools/distutils_patch.py:26: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
            "  \"Distutils was imported before Setuptools. This usage is discouraged \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "Setting LM training seed seed to 0\n",
            "Loading pretrained weights:  [PosixPath('/root/.fastai/models/es_multifit_paper_version/lm_best'), PosixPath('/root/.fastai/models/es_multifit_paper_version/itos')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/fastai/text/data.py:339: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero()\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  idx_min = (t != self.pad_idx).nonzero().min()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>idx</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>us ua ri ▁le ▁gu sta ▁confronta rse ▁con ▁los ▁ignora ntes ▁. ▁ ¿ ▁ xxmaj ▁va mos ▁a ▁estar ▁ tú ▁y ▁y o ▁de ▁acuerdo ▁o ▁no ▁ ? ▁( ▁discusión ▁) ▁18 ▁: ▁28 ▁27 ▁ dic ▁2014 ▁( ▁ xxup ▁ ut c ▁) ▁ xxbos ▁ xxfld ▁1 ▁ xxup ▁es ▁ xxup ▁notable mente ▁ xxup ▁ ten den ci oso ▁, ▁</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>cay s ▁los ▁cuales ▁el ▁gobierno ▁federal ▁no ▁ha ▁hecho ▁absoluta mente ▁nada ▁por ▁un ▁dinero ▁que ▁nadie ▁sabe ▁de ▁donde ▁viene ▁paga ndo ▁sus ▁viajes ▁por ▁la ▁rep ub lica ▁y ▁el ▁mundo ▁, ▁de ▁donde ▁sale ▁su ▁dinero ▁, ▁que ▁paso ▁con ▁el ▁2 ▁piso ▁de ▁la ▁ciudad ▁de ▁me xi co ▁, ▁de jo ▁a ▁su ▁ ami gui ta ▁ xxmaj ▁roble s ▁y ▁a ▁su</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>▁usa ▁como ▁medio ▁informa tivo ▁más ▁que ▁con tribu tivo ▁. ▁ xxmaj ▁salud os ▁y ▁dis cul pa ▁. ▁1 90 .2 . 41 ▁. ▁ xxbos ▁ xxfld ▁1 ▁ xxmaj ▁claro ▁, ▁pero ▁fue ▁otro ▁de ▁los ▁errores ▁de ▁ ba che let ▁esperar ▁3 ▁días ▁para ▁impone r ▁ to que ▁de ▁queda ▁y ▁dejar ▁que ▁se ▁produjer an ▁los ▁saque os ▁. ▁ xxmaj ▁la</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>ta pone s ▁de ▁punta ▁» ▁desde ▁el ▁comienzo ▁, ▁pre fe ría ▁saber ▁primero ▁qué ▁sucedió ▁desde ▁entonces ▁. ▁ xxmaj ▁si ▁me ▁dice s ▁que ▁por ▁ahora ▁, ▁las ▁aguas ▁están ▁cal mas ▁, ▁pre fi ero ▁no ▁hacer ▁ ola s ▁. ▁ xxmaj ▁después ▁de ▁todo ▁, ▁ ten go ▁muchas ▁otras ▁cosas ▁qué ▁hacer ▁. ▁ xxmaj ▁no ▁me ▁gu sta ría ▁perder ▁el ▁tiempo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>▁y ▁ tú ▁igual ▁de ▁ ne cio ▁con ▁eso ▁de ▁y o ▁solo ▁se ré ▁propuesto ▁por ▁ xxmaj ▁si abe f ▁, ▁gracias ▁. ▁ xxmaj ▁ er es ▁mal o ▁, ▁y ▁será s ▁castig ado ▁por ▁todos ▁ tus ▁pecado s ▁. ▁ xxmaj ▁ je je je ▁ xxmaj ▁co bal t ▁, ▁ → ▁mensaje s ▁ ← ▁ 00 ▁: ▁19 ▁2 ▁fe</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvpq-DCgA3od",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "236eb1d8-f195-4219-b45c-682faefd86c2"
      },
      "source": [
        "learn.fit_one_cycle(10,max_lr=1e-4)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>6.657520</td>\n",
              "      <td>4.536457</td>\n",
              "      <td>0.287893</td>\n",
              "      <td>31:34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>6.567366</td>\n",
              "      <td>4.486711</td>\n",
              "      <td>0.290306</td>\n",
              "      <td>31:24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>6.427095</td>\n",
              "      <td>4.419038</td>\n",
              "      <td>0.297321</td>\n",
              "      <td>31:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>6.352662</td>\n",
              "      <td>4.351205</td>\n",
              "      <td>0.304454</td>\n",
              "      <td>31:15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>6.167935</td>\n",
              "      <td>4.306272</td>\n",
              "      <td>0.306112</td>\n",
              "      <td>31:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>5.862950</td>\n",
              "      <td>4.273221</td>\n",
              "      <td>0.307238</td>\n",
              "      <td>31:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>5.923052</td>\n",
              "      <td>4.254035</td>\n",
              "      <td>0.307409</td>\n",
              "      <td>31:16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>5.919017</td>\n",
              "      <td>4.237604</td>\n",
              "      <td>0.308386</td>\n",
              "      <td>31:25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>5.811818</td>\n",
              "      <td>4.231569</td>\n",
              "      <td>0.308886</td>\n",
              "      <td>31:40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>5.980364</td>\n",
              "      <td>4.228451</td>\n",
              "      <td>0.308790</td>\n",
              "      <td>31:38</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2RkUhVOVK6s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f722be5-9c6d-473a-c530-60585a5c4608"
      },
      "source": [
        "self=exp.finetune_lm\n",
        "CLS_BEST = 'cls_best'\n",
        "LM_BEST = \"lm_best\"\n",
        "ENC_BEST = \"enc_best\"\n",
        "experiment_path=Path(path)\n",
        "self.experiment_path=experiment_path\n",
        "tokenizer = self.base.tokenizer\n",
        "print(\"Experiment\", experiment_path)\n",
        "\n",
        "self.experiment_path = experiment_path\n",
        "tokenizer.save(self.experiment_path, learn=learn)\n",
        "learn.to_fp32()\n",
        "learn.save_encoder('/content/'+ENC_BEST)\n",
        "learn.save(LM_BEST, with_opt=False)\n",
        "# learn.destroy()\n",
        "self.save_paramters()\n",
        "print(\"Language model saved to\", self.experiment_path)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experiment /content/gdrive/My Drive/multilingual toxic\n",
            "Copy sp model from /root/.fastai/models/es_multifit_paper_version to /content/gdrive/My Drive/multilingual toxic\n",
            "Saving dump to /content/gdrive/My Drive/multilingual toxic/finetuning.json\n",
            "Language model saved to /content/gdrive/My Drive/multilingual toxic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHv5lf-Th88k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "372d363d-6292-44e2-b740-d2faa7dca5c5"
      },
      "source": [
        "self=exp.classifier\n",
        "self.base.experiment_path=Path('')\n",
        "from pathlib import Path\n",
        "with strategy.scope():\n",
        "    exp.classifier.experiment_path=Path('')\n",
        "    data_clas = TextClasDataBunch.from_df(\"\", trn.reset_index(drop=True),val,  text_cols = 'content', label_cols = 'toxic',bs=32,vocab=data_lm.vocab)\n",
        "    data_clas.vocab.itos = data_lm.vocab.itos\n",
        "    l1rn = exp.classifier.get_learner(data_clas)  \n",
        "# learn is a preconfigured fastai learner with a pretrained model loaded\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using Label smoothing with eps =  0.05\n",
            "Setting Classifier weights seed seed to 0\n",
            "Training args:  {'drop_mult': 0.5, 'wd': 0.01, 'pretrained': False, 'bptt': 70, 'loss_func': FlattenedLoss of LabelSmoothingCrossEntropy(), 'clip': 0.12, 'silent': False} config:  {'emb_sz': 400, 'n_hid': 1550, 'n_layers': 4, 'pad_token': 1, 'qrnn': True, 'bidir': False, 'output_p': 0.25, 'hidden_p': 0.1, 'input_p': 0.2, 'embed_p': 0.02, 'weight_p': 0.15}\n",
            "Loading pretrained model /content/enc_best\n",
            "Setting Classifier training seed seed to 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysPVBeGziCCx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b3103d-70f9-420b-ef35-b533716f9786"
      },
      "source": [
        "data_clas.show_batch()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>xxbos xxup xxunk xxup xxunk xxup es xxup un xxup xxunk xxup que xxup no xxup xxunk xxup xxunk xxup xxunk xxup xxunk xxup en xxup su xxup xxunk xxup que xxup xxunk a xxup xxunk xxup de xxup la xxup xxunk , xxup ella xxup xxunk xxup que xxup en xxup xxunk xxup xxunk xxup le xxup ha xxup xxunk xxup un xxup xxunk xxup xxunk xxup xxunk .</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos xxup xxunk xxup xxunk . xxup xxunk xxup tu xxup xxunk xxup que xxup xxunk xxup xxunk xxup el xxup xxunk xxup xxunk xxup que xxup xxunk xxup de xxup los xxup xxunk y xxup xxunk xxup xxunk xxup xxunk xxup una xxup xxunk xxup que xxup xxunk xxup de xxup xxunk xxup xxunk xxup por xxup xxunk xxup xxunk xxup xxunk xxup en xxup su xxup xxunk xxup</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos no mas xxunk xxunk xxunk xxunk una xxunk , xxunk , xxunk , xxunk xxunk oh xxunk xxup xxunk xxunk xxup xxunk xxup xxunk xxup en xxup xxunk xxunk xxup xxunk xxup ah xxup xxunk xxup xxunk xxunk xxup nos xxup mas xxup xxunk xxup en xxup las xxup xxunk xxup de xxup la xxup xxunk , xxup oh xxup de xxup xxunk y xxup xxunk xxup el xxup</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos xxup xxunk ! ! ! xxup xxunk xxup xxunk xxup no xxup se xxup que xxup xxunk a xxup xxunk d xxup xxunk xxup xxunk xxup xxunk xxup es xxup el xxup mas xxup xxunk xxup yo xxup lo xxup xxunk xxup xxunk y xxup los q xxup no q xxup se xxup xxunk ! ! ! xxup yo xxup la xxup xxunk xxup xxunk xxup mi xxup xxunk</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos xxup xxunk xxup xxunk xxup xxunk xxup te xxup pedi xxup xxunk xxup que xxup me xxup xxunk xxup en xxup xxunk y xxup te xxup di xxup la xxup xxunk xxup xxunk xxup no xxup xxunk xxup xxunk xxup en xxup ya xxup me xxup as xxup xxunk xxup te xxup ve xxup xxunk xxup xxunk xxup de xxup xxunk xxup xxunk xxup yo xxup anda xxup xxunk</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8oz3uSbiDJS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "750461ff-a2a5-41d4-ec1b-c85a382207fd"
      },
      "source": [
        "l1rn.fit_one_cycle(10,max_lr=1e-3)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.659646</td>\n",
              "      <td>0.577690</td>\n",
              "      <td>0.744970</td>\n",
              "      <td>08:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.575238</td>\n",
              "      <td>0.543167</td>\n",
              "      <td>0.760355</td>\n",
              "      <td>09:54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.568608</td>\n",
              "      <td>0.575293</td>\n",
              "      <td>0.752663</td>\n",
              "      <td>08:44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.561401</td>\n",
              "      <td>0.520373</td>\n",
              "      <td>0.772189</td>\n",
              "      <td>09:32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.549595</td>\n",
              "      <td>0.513061</td>\n",
              "      <td>0.772781</td>\n",
              "      <td>09:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.537317</td>\n",
              "      <td>0.515119</td>\n",
              "      <td>0.779882</td>\n",
              "      <td>08:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.538268</td>\n",
              "      <td>0.515582</td>\n",
              "      <td>0.774556</td>\n",
              "      <td>09:41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.535492</td>\n",
              "      <td>0.505719</td>\n",
              "      <td>0.778698</td>\n",
              "      <td>09:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.528776</td>\n",
              "      <td>0.517313</td>\n",
              "      <td>0.775148</td>\n",
              "      <td>09:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.541324</td>\n",
              "      <td>0.505279</td>\n",
              "      <td>0.779290</td>\n",
              "      <td>08:08</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07KUT8p1B370",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9006b1e4-50b1-43f6-fc49-aadd4b9636f7"
      },
      "source": [
        "pre=l1rn.get_preds()[0][:,1].reshape(-1,1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oViudYCCBpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val['toxic']=np.asarray(pre)\n",
        "val.to_csv(path+'es_5.csv')"
      ],
      "execution_count": 19,
      "outputs": []
    }
  ]
}