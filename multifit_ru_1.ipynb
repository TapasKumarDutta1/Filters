{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multifit_ru_1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/multilingial/blob/master/multifit_ru_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Zg7Z1MxwxST",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "07ebab6d-2456-4d3b-8a05-335fe75f430d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtuVe1YOZSm3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "outputId": "751610e0-1fbf-4693-d3fb-721e8477311c"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\r\u001b[K     |▍                               | 10kB 21.4MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 1.9MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40kB 2.8MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51kB 2.4MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61kB 2.7MB/s eta 0:00:01\r\u001b[K     |███                             | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |███▍                            | 81kB 3.2MB/s eta 0:00:01\r\u001b[K     |███▉                            | 92kB 3.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |████▊                           | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 153kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 163kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 174kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 184kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 194kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 204kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 215kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 225kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 235kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 245kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 256kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 266kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 276kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 286kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 296kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 307kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 317kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 327kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 337kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 348kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 358kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 368kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 378kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 389kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 399kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 409kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 419kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 430kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 440kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 450kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 460kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 471kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 481kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 491kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 501kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 512kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 522kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 532kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 542kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 552kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 563kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 573kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 583kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 593kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 604kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 614kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 624kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 634kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 645kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 655kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 665kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 675kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 686kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 696kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 706kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 716kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 727kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 737kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 747kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 757kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 768kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 778kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 18.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 34.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 35.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=d60d3871657b4e715b6030b7a53b4eabe539f381efacd8b630abcbb756a3a397\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2idGzXgEV5V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "17f26c0c-342a-47d3-dda2-cdf535c3918f"
      },
      "source": [
        "pip install git+https://github.com/n-waves/multifit"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/n-waves/multifit\n",
            "  Cloning https://github.com/n-waves/multifit to /tmp/pip-req-build-xiiygk9p\n",
            "  Running command git clone -q https://github.com/n-waves/multifit /tmp/pip-req-build-xiiygk9p\n",
            "Building wheels for collected packages: multifit\n",
            "  Building wheel for multifit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for multifit: filename=multifit-1.0-cp36-none-any.whl size=24506 sha256=5ef7cadb98680b498bea1e2a6c820cf0b544789f15622021deb09a5f5c918e2a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-l4sydsjg/wheels/b7/92/1e/246f31a4e84fd665b5907cb96765f696be45e814ff68a5fd4a\n",
            "Successfully built multifit\n",
            "Installing collected packages: multifit\n",
            "Successfully installed multifit-1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4YeJJ3DE2YA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "c2667232-e9fa-417e-c2c0-c74f511251f2"
      },
      "source": [
        "pip install sacremoses"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (0.0.43)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sacremoses) (4.41.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMYSglC4GCBl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e3422225-1d37-4cc8-9a64-8c2f32a27bdb"
      },
      "source": [
        "pip install sentencepiece"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.91)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apj6OOR7JtnE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "60d470e4-abfa-4a5b-a0ce-85c9adc5f48d"
      },
      "source": [
        "from zipfile import ZipFile \n",
        "path = F\"/content/gdrive/My Drive/multilingual toxic/\" \n",
        "with ZipFile(path+'toxic.zip', 'r') as zip: \n",
        "    # printing all the contents of the zip file \n",
        "    zip.printdir() \n",
        "  \n",
        "    # extracting all the files \n",
        "    print('Extracting all the files now...') \n",
        "    zip.extractall() \n",
        "    print('Done!')\n",
        "import pandas as pd\n",
        "def load_data():\n",
        "    trn=pd.read_csv('jigsaw-toxic-comment-train.csv',usecols=['toxic','comment_text'])\n",
        "    val=pd.read_csv('validation.csv',usecols=['toxic','comment_text','lang'])\n",
        "    tst=pd.read_csv('test.csv',usecols=['lang','content'])  \n",
        "    sub=pd.read_csv('submission (1).csv')  \n",
        "    return trn,tst,val,sub\n",
        "train,test,valid,sub=load_data()\n",
        "test['toxic']=sub['toxic'].round()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Name                                             Modified             Size\n",
            "jigsaw-toxic-comment-train.csv                 2020-06-08 23:38:40     95538001\n",
            "test.csv                                       2020-06-08 23:39:02     28783206\n",
            "validation.csv                                 2020-06-08 23:38:52      3178555\n",
            "submission (1).csv                             2020-06-08 23:50:22      1161879\n",
            "Extracting all the files now...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OZ0ykdiRV6L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "10cd042d-9613-4a57-f0ed-2442750441b4"
      },
      "source": [
        "print(train.shape,test.shape,valid.shape,sub.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(223549, 2) (63812, 3) (8000, 3) (63812, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvS019LgKbjH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "5cd03350-366a-4798-a69d-69745dd72aae"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import transformers\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "from tqdm.notebook import tqdm\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
        "from tensorflow.keras.layers import *\n",
        "from fastai.text import *\n",
        "import multifit\n",
        "exp = multifit.from_pretrained(\"es_multifit_paper_version\")\n",
        "def regular_encode(texts, tokenizer, maxlen=512):\n",
        "    enc_di = tokenizer.batch_encode_plus(\n",
        "        texts, \n",
        "        return_attention_masks=False, \n",
        "        return_token_type_ids=False,\n",
        "        pad_to_max_length=True,\n",
        "        max_length=maxlen\n",
        "    )\n",
        "    \n",
        "    return np.array(enc_di['input_ids'])\n",
        "fa_config =  exp.pretrain_lm.tokenizer.get_fastai_config(add_open_file_processor=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/n-waves/multifit-models/releases/download/es_multifit_paper_version/es_multifit_paper_version.tgz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEikVSPlQkGv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "3c9b9959-a38b-430c-a8de-4f70906c2de8"
      },
      "source": [
        "es=test.loc[test['lang']=='ru']\n",
        "es['id']=es.index\n",
        "es=es.reset_index(drop=True)\n",
        "es.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>lang</th>\n",
              "      <th>toxic</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Вполне возможно, но я пока не вижу необходимо...</td>\n",
              "      <td>ru</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>, под вашу ответственность. NB: ЭСБЕ, значит, ...</td>\n",
              "      <td>ru</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ошибаетесь Вы. Во-первых, текст должен быть п...</td>\n",
              "      <td>ru</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Отличная работа, спасибо. Я чуть доработал - п...</td>\n",
              "      <td>ru</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>левакофашист и путинский лизун ??? Как вообще ...</td>\n",
              "      <td>ru</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content lang  toxic  id\n",
              "0   Вполне возможно, но я пока не вижу необходимо...   ru    0.0   1\n",
              "1  , под вашу ответственность. NB: ЭСБЕ, значит, ...   ru    0.0  18\n",
              "2   Ошибаетесь Вы. Во-первых, текст должен быть п...   ru    0.0  20\n",
              "3  Отличная работа, спасибо. Я чуть доработал - п...   ru    0.0  26\n",
              "4  левакофашист и путинский лизун ??? Как вообще ...   ru    0.0  28"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvuqbUaHZtUq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "721a1e58-8d58-47fd-8ccf-d872a5e11df3"
      },
      "source": [
        "# Detect hardware, return appropriate distribution strategy\n",
        "try:\n",
        "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
        "    # set: this is always the case on Kaggle.\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running on TPU  grpc://10.31.86.186:8470\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.31.86.186:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.31.86.186:8470\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeJdr-HQA_BX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "es['toxic']=es['toxic'].astype(np.long)\n",
        "ln=es.shape[0]//5\n",
        "trn=es.loc[ln:]\n",
        "val=es.loc[:ln]\n",
        "print(trn.shape,val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4eDzDHjY77y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "    exp = multifit.from_pretrained(\"fr_multifit_paper_version\")\n",
        "    fa_config =  exp.pretrain_lm.tokenizer.get_fastai_config(add_open_file_processor=True)\n",
        "    data_lm = (TextList.from_df(es, **fa_config)\n",
        "            .split_by_rand_pct(0.1)\n",
        "            .label_for_lm()           \n",
        "            .databunch(bs=32))\n",
        "    learn = exp.finetune_lm.get_learner(data_lm)  \n",
        "    # learn is a preconfigured fastai learner with a pretrained model loaded\n",
        "data_lm.show_batch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvpq-DCgA3od",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.fit_one_cycle(10,max_lr=1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2RkUhVOVK6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "self=exp.finetune_lm\n",
        "CLS_BEST = 'cls_best'\n",
        "LM_BEST = \"lm_best\"\n",
        "ENC_BEST = \"enc_best\"\n",
        "experiment_path=Path(path)\n",
        "self.experiment_path=experiment_path\n",
        "tokenizer = self.base.tokenizer\n",
        "print(\"Experiment\", experiment_path)\n",
        "\n",
        "self.experiment_path = experiment_path\n",
        "tokenizer.save(self.experiment_path, learn=learn)\n",
        "learn.to_fp32()\n",
        "learn.save_encoder('/content/'+ENC_BEST)\n",
        "learn.save(LM_BEST, with_opt=False)\n",
        "# learn.destroy()\n",
        "self.save_paramters()\n",
        "print(\"Language model saved to\", self.experiment_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHv5lf-Th88k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "self=exp.classifier\n",
        "self.base.experiment_path=Path('')\n",
        "from pathlib import Path\n",
        "with strategy.scope():\n",
        "    exp.classifier.experiment_path=Path('')\n",
        "    data_clas = TextClasDataBunch.from_df(\"\", trn.reset_index(drop=True),val,  text_cols = 'content', label_cols = 'toxic',bs=32,vocab=data_lm.vocab)\n",
        "    data_clas.vocab.itos = data_lm.vocab.itos\n",
        "    l1rn = exp.classifier.get_learner(data_clas)  \n",
        "# learn is a preconfigured fastai learner with a pretrained model loaded\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysPVBeGziCCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_clas.show_batch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8oz3uSbiDJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l1rn.fit_one_cycle(10,max_lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07KUT8p1B370",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pre=l1rn.get_preds()[0][:,1].reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oViudYCCBpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val['toxic']=np.asarray(pre)\n",
        "val.to_csv(path+'ru_1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}