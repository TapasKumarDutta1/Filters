{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "portuguese_all",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/multilingial/blob/master/portuguese_all.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e13e02c6-0634-48e8-9015-be06695a7fe9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV04VUZxKYQH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caad6e6f-39a3-4ca1-d451-ff6646a3d26a"
      },
      "source": [
        "pip install sentencepiece "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 6.6MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd3b2666-43ca-4861-965d-84b6d7a40bb9"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 21.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 56.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=ab7888bd465a58e4f2920d6607991c4b669646e0e7c9a2c4390f2501d3c9ebaa\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26af9eb4-03e2-40d6-8d96-e0aa1cfc493e"
      },
      "source": [
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  5116  100  5116    0     0  26785      0 --:--:-- --:--:-- --:--:-- 26926\n",
            "Updating... This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-nightly ...\n",
            "Collecting cloud-tpu-client\n",
            "  Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl\n",
            "Uninstalling torch-1.7.0+cu101:\n",
            "Collecting google-api-python-client==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client) (4.1.3)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.17.2)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.17.4)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.16.0)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.15.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.0.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (4.2.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (51.3.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2018.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.52.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.12.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2020.12.5)\n",
            "Installing collected packages: google-api-python-client, cloud-tpu-client\n",
            "  Found existing installation: google-api-python-client 1.7.12\n",
            "    Uninstalling google-api-python-client-1.7.12:\n",
            "      Successfully uninstalled google-api-python-client-1.7.12\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0\n",
            "Done updating TPU runtime\n",
            "  Successfully uninstalled torch-1.7.0+cu101\n",
            "Uninstalling torchvision-0.8.1+cu101:\n",
            "  Successfully uninstalled torchvision-0.8.1+cu101\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][123.1 MiB/123.1 MiB]                                                \n",
            "Operation completed over 1 objects/123.1 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "\\ [1 files][132.0 MiB/132.0 MiB]                                                \n",
            "Operation completed over 1 objects/132.0 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  4.9 MiB/  4.9 MiB]                                                \n",
            "Operation completed over 1 objects/4.9 MiB.                                      \n",
            "Processing ./torch-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly) (1.19.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==nightly) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==nightly) (3.7.4.3)\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.8.0a0+unknown\n",
            "Processing ./torch_xla-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "Successfully installed torch-xla-1.6+774f493\n",
            "Processing ./torchvision-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (7.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (1.8.0a0+unknown)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (1.19.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly) (3.7.4.3)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.9.0a0+e04de77\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libopenblas-dev is already the newest version (0.2.20+ds-4).\n",
            "The following NEW packages will be installed:\n",
            "  libomp5\n",
            "0 upgraded, 1 newly installed, 0 to remove and 13 not upgraded.\n",
            "Need to get 234 kB of archives.\n",
            "After this operation, 774 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Fetched 234 kB in 1s (305 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 146374 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG"
      },
      "source": [
        "import gc\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datetime import date\n",
        "from transformers import *\n",
        "from sklearn.metrics import *\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from torch.optim import *\n",
        "from torch.nn.modules.loss import *\n",
        "from torch.optim.lr_scheduler import * \n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import RandomSampler\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "def regular_encode(texts, tokenizer, maxlen=192):\n",
        "    enc_di = tokenizer.batch_encode_plus(\n",
        "        texts, \n",
        "        return_token_type_ids=False,\n",
        "        pad_to_max_length=True,\n",
        "        max_length=maxlen\n",
        "    )\n",
        "    \n",
        "    return np.array(enc_di['input_ids'])\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, transformer, num_classes=1):\n",
        "        \"\"\"\n",
        "        Constructor\n",
        "        \n",
        "        Arguments:\n",
        "            model {string} -- Transformer to build the model on. Expects \"camembert-base\".\n",
        "            num_classes {int} -- Number of classes (default: {1})\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        self.transformer = transformer\n",
        "\n",
        "        self.nb_features = self.transformer.pooler.dense.out_features\n",
        "        # for param in self.transformer.parameters():\n",
        "        #   param.requires_grad = False\n",
        "        self.pooler = nn.Sequential(\n",
        "            nn.Linear(self.nb_features, num_classes), \n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        \"\"\"\n",
        "        Usual torch forward function\n",
        "        \n",
        "        Arguments:\n",
        "            tokens {torch tensor} -- Sentence tokens\n",
        "        \n",
        "        Returns:\n",
        "            torch tensor -- Class logits\n",
        "        \"\"\"\n",
        "        hidden_states = self.transformer(\n",
        "            tokens, attention_mask=(tokens > 0).long()\n",
        "        )[1]\n",
        "\n",
        "        # hidden_states = hidden_states[-1][:, 0] # Use the representation of the first token of the last layer\n",
        "\n",
        "        ft = self.pooler(hidden_states)\n",
        "\n",
        "        return ft\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4"
      },
      "source": [
        "class bce(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(bce, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        \n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        \n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        \n",
        "        one=(1-targets)*torch.log(1-inputs)\n",
        "        zero=(targets*torch.log(inputs))\n",
        "        loss = torch.mean((one+zero)*-1)\n",
        "        \n",
        "        return loss\n",
        "class JigsawDataset:\n",
        "    \"\"\"\n",
        "    Torch dataset for training and validating\n",
        "    \"\"\"\n",
        "    def __init__(self, x,y,is_test):\n",
        "        super().__init__()\n",
        "        self.y = y \n",
        "        self.is_test=is_test\n",
        "        self.sentences = x\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.sentences.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      len=self.__len__()\n",
        "      if idx>len:\n",
        "        idx=idx%len\n",
        "      if self.is_test==0:\n",
        "        return torch.tensor(self.sentences[idx]), torch.tensor(self.y[idx]).float()\n",
        "      else:\n",
        "        return torch.tensor(self.sentences[idx])\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from statistics import mean\n",
        "import torch_xla\n",
        "from sklearn.preprocessing import *\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.data_parallel as dp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.utils.utils as xu\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.test.test_utils as test_utils\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import scipy as sp\n",
        "import gc\n",
        "import os\n",
        "import cv2\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import random\n",
        "import argparse\n",
        "import sys\n",
        "from statistics import mean\n",
        "import yaml\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import albumentations as A\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import random\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "from typing import Dict\n",
        "from tempfile import gettempdir\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models.resnet import resnet50, resnet18, resnet34, resnet101\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import KFold\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def train_all(train_loader, model, device, optimizer):\n",
        "    model.train()\n",
        "    # t = tqdm(train_loader, disable=not xm.is_master_ordinal())\n",
        "    model.train()\n",
        "    lss=bce()\n",
        "    loss1=[]\n",
        "    for step, (x, y_batch) in enumerate(train_loader): \n",
        "            \n",
        "            # x = x.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            y_pred = model(x)\n",
        "            \n",
        "            loss = lss(y_pred.view(-1).float(), y_batch.float())\n",
        "            loss.backward()\n",
        "            loss1.append(loss.item())\n",
        "            xm.optimizer_step(optimizer)\n",
        "            \n",
        "            model.zero_grad()\n",
        "    return mean(loss1)\n",
        "\n",
        "def valid_all(train_loader, model, device):\n",
        "    # t = tqdm(train_loader, disable=not xm.is_master_ordinal())\n",
        "    lss=bce()\n",
        "    loss1=[]\n",
        "    for step, (x, y_batch) in enumerate(train_loader): \n",
        "            \n",
        "            # x = x.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            y_pred = model(x)\n",
        "            \n",
        "            loss = lss(y_pred.view(-1).float(), y_batch.float())\n",
        "            loss1.append(loss.item())\n",
        "            \n",
        "    return mean(loss1)\n",
        "\n",
        "def predict_all(train_loader, model,device):\n",
        "    # t = tqdm(train_loader, disable=not xm.is_master_ordinal())\n",
        "    predict=[]\n",
        "    for step, (x) in tqdm(enumerate(train_loader)): \n",
        "            \n",
        "            y_pred = model(x.to(device))\n",
        "            predict.append(y_pred)\n",
        "            \n",
        "    return predict\n",
        "\n",
        "\n",
        "def load_data(lang):\n",
        "    with zipfile.ZipFile('/content/gdrive/My Drive/multilingual/jigsaw-toxic-comment-train-google-'+lang+'-cleaned.csv.zip', 'r') as zip_ref:\n",
        "      zip_ref.extractall('')\n",
        "    trn=pd.read_csv('/content/jigsaw-toxic-comment-train-google-'+lang+'-cleaned.csv',usecols=['toxic','comment_text'])\n",
        "    trn['lang']=lang\n",
        "    tst=pd.read_csv('/content/gdrive/My Drive/multilingual/test.csv.zip',usecols=['lang','content'])  \n",
        "    sub=pd.read_csv('/content/gdrive/My Drive/multilingual/submission.csv')\n",
        "    val=pd.read_csv( '/content/gdrive/My Drive/multilingual/validation.csv.zip',usecols=['lang','comment_text','toxic'])  \n",
        "    tst.columns=['comment_text','lang']\n",
        "    tst['toxic']=sub['toxic']\n",
        "    df=pd.concat([trn,tst,val],0)\n",
        "    return df.loc[df['lang']==lang].reset_index(drop=True).drop(['lang'],1)\n",
        "\n",
        "\n",
        "def get_lang(val,tst,lang):\n",
        "  df=pd.concat([val,tst],0)\n",
        "  return df.loc[df['lang']==lang].reset_index(drop=True).drop(['id','lang'],1)\n",
        "\n",
        "def main():\n",
        "    epochs=1\n",
        "    batch_size=16\n",
        "    learning_rate=1e-5\n",
        "    seed=42\n",
        "\n",
        "    # Setting seed\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    df=load_data('pt')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
        "    x_train = regular_encode(list(df.comment_text.values), tokenizer, maxlen=192)\n",
        "    y_train = df.toxic.values\n",
        "\n",
        "    idx=df.loc[(df['toxic']>0) & (df['toxic']<1)].index\n",
        "    test=x_train[idx]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def run():\n",
        "\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "        device = xm.xla_device()\n",
        "        model = AutoModel.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
        "        model=Transformer(model).to(device)\n",
        "\n",
        "\n",
        "        #Training\n",
        "        train_dataset = JigsawDataset(trn_x,trn_y,0)\n",
        "\n",
        "\n",
        "        train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "            train_dataset,\n",
        "            num_replicas=xm.xrt_world_size(),\n",
        "            rank=xm.get_ordinal(),\n",
        "            shuffle=False\n",
        "        )\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,\n",
        "            sampler=train_sampler,\n",
        "            drop_last=False,\n",
        "            num_workers=2\n",
        "        )\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate * xm.xrt_world_size(),weight_decay=1e-3)\n",
        "\n",
        "\n",
        "        xm.master_print(\"Training is Starting ...... \")\n",
        "        total_loss=[]\n",
        "        valid_loss=[]\n",
        "        predictions=[]\n",
        "        for i in tqdm(range(3)):\n",
        "          para_loader = pl.ParallelLoader(train_loader, [device])\n",
        "          total_loss.append(train_all(para_loader.per_device_loader(device),model,device,optimizer))\n",
        "\n",
        "        \n",
        "        for i in tqdm(range(3)):\n",
        "          para_loader = pl.ParallelLoader(validation_loader, [device])\n",
        "          valid_loss.append(valid_all(para_loader.per_device_loader(device),model,device))\n",
        "\n",
        "\n",
        "\n",
        "        state = { 'state_dict': model.state_dict(),\n",
        "             'optimizer': optimizer.state_dict()}\n",
        "        xm.save(state, 'portuguese'+str(number))\n",
        "\n",
        "\n",
        "        # para_loader = pl.ParallelLoader(test_loader, [device])\n",
        "        # predictions=predict_all(para_loader.per_device_loader(device),model,device)\n",
        "        np.save('loss.npy',total_loss)\n",
        "        np.save('valid.npy',valid_loss)\n",
        "        # np.save('predictions.npy',predictions)\n",
        "            \n",
        "\n",
        "\n",
        "    def _mp_fn(rank, flags):\n",
        "        torch.set_default_tensor_type('torch.FloatTensor')\n",
        "        run()\n",
        "    kf=KFold(n_splits=5, random_state=42, shuffle=False)\n",
        "    number=0\n",
        "    for train_index, test_index in kf.split(range(df.shape[0])):\n",
        "      trn_x=x_train[train_index]\n",
        "      trn_y=y_train[train_index]\n",
        "      number+=1\n",
        "      FLAGS={}\n",
        "      xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd104da-d9b5-4db1-fa75-d30dfa239c16"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training is Starting ...... \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/3 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZN73Nb9siLd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "bddbf2df-00e9-4dc1-8d4e-fef82128636c"
      },
      "source": [
        "def train_all(train_loader, model, device, optimizer):\r\n",
        "    model.train()\r\n",
        "    # t = tqdm(train_loader, disable=not xm.is_master_ordinal())\r\n",
        "    model.train()\r\n",
        "    lss=bce()\r\n",
        "    loss1=[]\r\n",
        "    for step, (x, y_batch) in enumerate(train_loader): \r\n",
        "            \r\n",
        "            # x = x.to(device)\r\n",
        "            y_batch = y_batch.to(device)\r\n",
        "            y_pred = model(x)\r\n",
        "            \r\n",
        "            loss = lss(y_pred.view(-1).float(), y_batch.float())\r\n",
        "            loss.backward()\r\n",
        "            loss1.append(loss.item())\r\n",
        "            optimizer.step()\r\n",
        "            \r\n",
        "            model.zero_grad()\r\n",
        "    return mean(loss1)\r\n",
        "\r\n",
        "def valid_all(train_loader, model, device):\r\n",
        "    # t = tqdm(train_loader, disable=not xm.is_master_ordinal())\r\n",
        "    lss=bce()\r\n",
        "    loss1=[]\r\n",
        "    for step, (x, y_batch) in enumerate(train_loader): \r\n",
        "            \r\n",
        "            # x = x.to(device)\r\n",
        "            y_batch = y_batch.to(device)\r\n",
        "            y_pred = model(x)\r\n",
        "            \r\n",
        "            loss = lss(y_pred.view(-1).float(), y_batch.float())\r\n",
        "            loss1.append(loss.item())\r\n",
        "            \r\n",
        "    return mean(loss1)\r\n",
        "\r\n",
        "def predict_all(train_loader, model,device,df,batch_size):\r\n",
        "    # t = tqdm(train_loader, disable=not xm.is_master_ordinal())\r\n",
        "    predict=[]\r\n",
        "    for step, (x) in tqdm(enumerate(train_loader),total=df.shape[0]/(batch_size)): \r\n",
        "            \r\n",
        "            y_pred = model(x.to(device))\r\n",
        "            predict.append(y_pred.cpu().detach().numpy())\r\n",
        "            \r\n",
        "    return predict\r\n",
        "\r\n",
        "def load_data(lang):\r\n",
        "    tst=pd.read_csv('/content/gdrive/My Drive/multilingual/test.csv.zip',usecols=['lang','content']) \r\n",
        "    tst=tst.loc[tst['lang']==lang].reset_index(drop=True).drop(['lang'],1)\r\n",
        "    print(tst.shape)\r\n",
        "    # tst = tst.iloc[:1000]\r\n",
        "    return tst\r\n",
        "\r\n",
        "\r\n",
        "def get_lang(val,tst,lang):\r\n",
        "  df=pd.concat([val,tst],0)\r\n",
        "  return df.loc[df['lang']==lang].reset_index(drop=True).drop(['id','lang'],1)\r\n",
        "\r\n",
        "def main():\r\n",
        "    epochs=1\r\n",
        "    seed=42\r\n",
        "    batch_size=16\r\n",
        "    # Setting seed\r\n",
        "    random.seed(seed)\r\n",
        "    np.random.seed(seed)\r\n",
        "    torch.manual_seed(seed)\r\n",
        "    torch.cuda.manual_seed(seed)\r\n",
        "    torch.backends.cudnn.deterministic = True\r\n",
        "\r\n",
        "    df=load_data('es')\r\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\r\n",
        "    x_train = regular_encode(list(df.content.values), tokenizer, maxlen=192)\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    def run():\r\n",
        "\r\n",
        "        torch.manual_seed(seed)\r\n",
        "\r\n",
        "        device = xm.xla_device()\r\n",
        "        model = AutoModel.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\r\n",
        "        model=Transformer(model).to(device)\r\n",
        "        model.load_state_dict(torch.load( 'portuguese'+str(number))['state_dict'])\r\n",
        "\r\n",
        "        #Training\r\n",
        "        train_dataset = JigsawDataset(x_train,None,1)\r\n",
        "\r\n",
        "\r\n",
        "        train_sampler = torch.utils.data.distributed.DistributedSampler(\r\n",
        "            train_dataset,\r\n",
        "            num_replicas=xm.xrt_world_size(),\r\n",
        "            rank=xm.get_ordinal(),\r\n",
        "            shuffle=False\r\n",
        "        )\r\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,\r\n",
        "            sampler=train_sampler,\r\n",
        "            drop_last=False,\r\n",
        "            num_workers=2\r\n",
        "        )\r\n",
        "\r\n",
        "        \r\n",
        "\r\n",
        "\r\n",
        "        \r\n",
        "        predictions=[]\r\n",
        "        para_loader = pl.ParallelLoader(train_loader, [device])\r\n",
        "        predictions.append(predict_all(para_loader.per_device_loader(device),model,device,df,batch_size))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        np.save('/conetnt/gdrive/My Drive/multilingual/portuguese_predictions_'+str(number)+'.npy',predictions)\r\n",
        "            \r\n",
        "    for number in range(5):\r\n",
        "      run()\r\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fa7af8adf60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uywbMB3w2MZl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}