{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spanish_predictions.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/multilingial/blob/master/spanish_predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utc1m9Sh3-Gs"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')\r\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K20JnXUAsFKv"
      },
      "source": [
        "pip install sentencepiece "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9Kbsp-m4DOS"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRNY_bQdsGPK"
      },
      "source": [
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\r\n",
        "!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaSmfAuO8G5R"
      },
      "source": [
        "import gc\r\n",
        "import os\r\n",
        "import time\r\n",
        "import math\r\n",
        "import random\r\n",
        "import warnings\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "from datetime import date\r\n",
        "from transformers import *\r\n",
        "from sklearn.metrics import *\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.utils.data\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "from transformers import AutoTokenizer, AutoModel\r\n",
        "import torch.nn as nn\r\n",
        "from torch import Tensor\r\n",
        "from torch.optim import *\r\n",
        "from torch.nn.modules.loss import *\r\n",
        "from torch.optim.lr_scheduler import * \r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from torch.utils.data.sampler import RandomSampler\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def regular_encode(texts, tokenizer, maxlen=192):\r\n",
        "    enc_di = tokenizer.batch_encode_plus(\r\n",
        "        texts, \r\n",
        "        return_token_type_ids=False,\r\n",
        "        pad_to_max_length=True,\r\n",
        "        max_length=maxlen\r\n",
        "    )\r\n",
        "    \r\n",
        "    return np.array(enc_di['input_ids'])\r\n",
        "\r\n",
        "class Transformer(nn.Module):\r\n",
        "    def __init__(self, transformer, num_classes=1):\r\n",
        "        \"\"\"\r\n",
        "        Constructor\r\n",
        "        \r\n",
        "        Arguments:\r\n",
        "            model {string} -- Transformer to build the model on. Expects \"camembert-base\".\r\n",
        "            num_classes {int} -- Number of classes (default: {1})\r\n",
        "        \"\"\"\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.transformer = transformer\r\n",
        "\r\n",
        "        self.nb_features = self.transformer.pooler.dense.out_features\r\n",
        "        # for param in self.transformer.parameters():\r\n",
        "        #   param.requires_grad = False\r\n",
        "        self.pooler = nn.Sequential(\r\n",
        "            nn.Linear(self.nb_features, num_classes), \r\n",
        "            nn.Sigmoid(),\r\n",
        "        )\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, tokens):\r\n",
        "        \"\"\"\r\n",
        "        Usual torch forward function\r\n",
        "        \r\n",
        "        Arguments:\r\n",
        "            tokens {torch tensor} -- Sentence tokens\r\n",
        "        \r\n",
        "        Returns:\r\n",
        "            torch tensor -- Class logits\r\n",
        "        \"\"\"\r\n",
        "        hidden_states = self.transformer(\r\n",
        "            tokens, attention_mask=(tokens > 0).long()\r\n",
        "        )[1]\r\n",
        "\r\n",
        "        # hidden_states = hidden_states[-1][:, 0] # Use the representation of the first token of the last layer\r\n",
        "\r\n",
        "        ft = self.pooler(hidden_states)\r\n",
        "\r\n",
        "        return ft\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U8eO_Pz4LNX"
      },
      "source": [
        "class bce(nn.Module):\r\n",
        "    def __init__(self, weight=None, size_average=True):\r\n",
        "        super(bce, self).__init__()\r\n",
        "\r\n",
        "    def forward(self, inputs, targets, smooth=1):\r\n",
        "        \r\n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\r\n",
        "        \r\n",
        "        #flatten label and prediction tensors\r\n",
        "        inputs = inputs.view(-1)\r\n",
        "        targets = targets.view(-1)\r\n",
        "        \r\n",
        "        one=(1-targets)*torch.log(1-inputs)\r\n",
        "        zero=(targets*torch.log(inputs))\r\n",
        "        loss = torch.mean((one+zero)*-1)\r\n",
        "        \r\n",
        "        return loss\r\n",
        "class JigsawDataset:\r\n",
        "    \"\"\"\r\n",
        "    Torch dataset for training and validating\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, x,y,is_test):\r\n",
        "        super().__init__()\r\n",
        "        self.y = y \r\n",
        "        self.is_test=is_test\r\n",
        "        self.sentences = x\r\n",
        "        \r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return self.sentences.shape[0]\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "      len=self.__len__()\r\n",
        "      if idx>len:\r\n",
        "        idx=idx%len\r\n",
        "      if self.is_test==0:\r\n",
        "        return torch.tensor(self.sentences[idx]), torch.tensor(self.y[idx]).float()\r\n",
        "      else:\r\n",
        "        return torch.tensor(self.sentences[idx])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn5B5wzm5lST"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "from statistics import mean\r\n",
        "import torch_xla\r\n",
        "from sklearn.preprocessing import *\r\n",
        "import torch_xla.debug.metrics as met\r\n",
        "import torch_xla.distributed.data_parallel as dp\r\n",
        "import torch_xla.distributed.parallel_loader as pl\r\n",
        "import torch_xla.utils.utils as xu\r\n",
        "import torch_xla.core.xla_model as xm\r\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\r\n",
        "import torch_xla.test.test_utils as test_utils\r\n",
        "import numpy as np # linear algebra\r\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
        "import scipy as sp\r\n",
        "import gc\r\n",
        "import os\r\n",
        "import cv2\r\n",
        "import zipfile\r\n",
        "from pathlib import Path\r\n",
        "import random\r\n",
        "import argparse\r\n",
        "import sys\r\n",
        "from statistics import mean\r\n",
        "import yaml\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "import time\r\n",
        "import albumentations as A\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "import random\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "import pandas as pd\r\n",
        "from typing import Dict\r\n",
        "from tempfile import gettempdir\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import torch\r\n",
        "from torch import nn, optim\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torchvision.models.resnet import resnet50, resnet18, resnet34, resnet101\r\n",
        "from tqdm import tqdm\r\n",
        "import seaborn as sns\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "def train_all(train_loader, model, device, optimizer):\r\n",
        "    model.train()\r\n",
        "    # t = tqdm(train_loader, disable=not xm.is_master_ordinal())\r\n",
        "    model.train()\r\n",
        "    lss=bce()\r\n",
        "    loss1=[]\r\n",
        "    for step, (x, y_batch) in enumerate(train_loader): \r\n",
        "            \r\n",
        "            # x = x.to(device)\r\n",
        "            y_batch = y_batch.to(device)\r\n",
        "            y_pred = model(x)\r\n",
        "            \r\n",
        "            loss = lss(y_pred.view(-1).float(), y_batch.float())\r\n",
        "            loss.backward()\r\n",
        "            loss1.append(loss.item())\r\n",
        "            optimizer.step()\r\n",
        "            \r\n",
        "            model.zero_grad()\r\n",
        "    return mean(loss1)\r\n",
        "\r\n",
        "def valid_all(train_loader, model, device):\r\n",
        "    # t = tqdm(train_loader, disable=not xm.is_master_ordinal())\r\n",
        "    lss=bce()\r\n",
        "    loss1=[]\r\n",
        "    for step, (x, y_batch) in enumerate(train_loader): \r\n",
        "            \r\n",
        "            # x = x.to(device)\r\n",
        "            y_batch = y_batch.to(device)\r\n",
        "            y_pred = model(x)\r\n",
        "            \r\n",
        "            loss = lss(y_pred.view(-1).float(), y_batch.float())\r\n",
        "            loss1.append(loss.item())\r\n",
        "            \r\n",
        "    return mean(loss1)\r\n",
        "\r\n",
        "def predict_all(train_loader, model,device,df,batch_size):\r\n",
        "    # t = tqdm(train_loader, disable=not xm.is_master_ordinal())\r\n",
        "    predict=[]\r\n",
        "    for step, (x) in tqdm(enumerate(train_loader),total=df.shape[0]/(batch_size)): \r\n",
        "            \r\n",
        "            y_pred = model(x.to(device))\r\n",
        "            predict.append(y_pred.cpu().detach().numpy())\r\n",
        "            \r\n",
        "    return predict\r\n",
        "\r\n",
        "def load_data(lang):\r\n",
        "    tst=pd.read_csv('/content/gdrive/My Drive/multilingual/test.csv.zip',usecols=['lang','content']) \r\n",
        "    tst=tst.loc[tst['lang']==lang]\r\n",
        "    print(tst.shape)\r\n",
        "    tst = tst.iloc[:1000].reset_index(drop=True).drop(['lang'],1)\r\n",
        "    return tst\r\n",
        "\r\n",
        "\r\n",
        "def get_lang(val,tst,lang):\r\n",
        "  df=pd.concat([val,tst],0)\r\n",
        "  return df.loc[df['lang']==lang].reset_index(drop=True).drop(['id','lang'],1)\r\n",
        "\r\n",
        "def main():\r\n",
        "    epochs=1\r\n",
        "    seed=42\r\n",
        "    batch_size=16\r\n",
        "    # Setting seed\r\n",
        "    random.seed(seed)\r\n",
        "    np.random.seed(seed)\r\n",
        "    torch.manual_seed(seed)\r\n",
        "    torch.cuda.manual_seed(seed)\r\n",
        "    torch.backends.cudnn.deterministic = True\r\n",
        "\r\n",
        "    df=load_data('es')\r\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\r\n",
        "    x_train = regular_encode(list(df.content.values), tokenizer, maxlen=192)\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    def run():\r\n",
        "\r\n",
        "        torch.manual_seed(seed)\r\n",
        "\r\n",
        "        device = xm.xla_device()\r\n",
        "        model = AutoModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\r\n",
        "        model=Transformer(model).to(device)\r\n",
        "        model.load_state_dict(torch.load( '/content/gdrive/My Drive/spanish')['state_dict'])\r\n",
        "\r\n",
        "        #Training\r\n",
        "        train_dataset = JigsawDataset(x_train,None,1)\r\n",
        "\r\n",
        "\r\n",
        "        train_sampler = torch.utils.data.distributed.DistributedSampler(\r\n",
        "            train_dataset,\r\n",
        "            num_replicas=xm.xrt_world_size(),\r\n",
        "            rank=xm.get_ordinal(),\r\n",
        "            shuffle=False\r\n",
        "        )\r\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,\r\n",
        "            sampler=train_sampler,\r\n",
        "            drop_last=False,\r\n",
        "            num_workers=2\r\n",
        "        )\r\n",
        "\r\n",
        "        \r\n",
        "\r\n",
        "\r\n",
        "        \r\n",
        "        predictions=[]\r\n",
        "        para_loader = pl.ParallelLoader(train_loader, [device])\r\n",
        "        predictions.append(predict_all(para_loader.per_device_loader(device),model,device,df,batch_size))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        np.save('predictions.npy',predictions)\r\n",
        "            \r\n",
        "\r\n",
        "    run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T9c_puh6NOW"
      },
      "source": [
        "\r\n",
        "a=np.load('predictions.npy',allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWcOyDIqInY5"
      },
      "source": [
        "ls=[]\r\n",
        "for i in range(len(a[0])):\r\n",
        "  ls.extend(a[0][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFTdSJmu7_1m"
      },
      "source": [
        "np.save('/content/gdrive/My Drive/multilingual/spanish_predictions_1000.npy',[l.tolist()[0] for l in ls])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFsQiSNqOyud"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}