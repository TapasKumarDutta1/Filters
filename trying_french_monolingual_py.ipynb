{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trying_french_monolingual.py",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/multilingial/blob/master/trying_french_monolingual_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtjP1obcBbL8",
        "outputId": "f81627b9-e38e-425e-ca98-c04a8dca15a0"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')\r\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ObeBkJD06Zu",
        "outputId": "5601aaf8-10b5-4934-d7a2-c66f5dfda6a8"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/40/866cbfac4601e0f74c7303d533a9c5d4a53858bd402e08e3e294dd271f25/transformers-4.2.1-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 5.1MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 17.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 20.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=8891652081e031c023f8cacfd398237c2bcdba635c1c517ba032dc5bdb6b2110\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82jsWtZA1SP6",
        "outputId": "00af51f3-72e2-4148-db0f-12dd773e5fee"
      },
      "source": [
        "pip install sentencepiece"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\r\u001b[K     |▎                               | 10kB 16.3MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 11.5MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 8.1MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 8.2MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 4.5MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 5.0MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 5.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 5.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 5.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 5.8MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 5.8MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 5.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 5.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 5.8MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 5.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 5.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 5.8MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X936fx7_qxwx",
        "outputId": "e1064b1b-f935-4a84-b8b8-b6dc66a90093"
      },
      "source": [
        "pip install wandb"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/07/f874b16aa864e01a3ed4051dda467a3a16fc9419dae549689800cfbb17dd/wandb-0.10.14-py2.py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.15.0)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/08/b2/ef713e0e67f6e7ec7d59aea3ee78d05b39c15930057e724cc6d362a8c3bb/configparser-5.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/5c/018bf9a5c24343a664deaea70e61f33f53bb1bd3caf193110f827bfd07e2/sentry_sdk-0.19.5-py2.py3-none-any.whl (128kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 35.3MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/cb/ec98155c501b68dcb11314c7992cd3df6dce193fd763084338a117967d53/GitPython-3.1.12-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 31.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting watchdog<0.10.5,>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/10/500580a0987363a0d9e1f3dd5cb1bba94a47e19266c6ce9dfb6cdd455758/watchdog-0.10.4.tar.gz (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.12.0->wandb) (51.1.1)\n",
            "Collecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, watchdog, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6490 sha256=ec0ddb1b5b461583e87a20f77b777eb12e0bc8948115502b194845ecc518bb11\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.4-cp36-none-any.whl size=74842 sha256=876f91c524185005ac82922599def346b2067c87a12a7e8dff52be3e93713011\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/11/04/5160b8815b0cc7cf574bdc6d053e510169ec264c8791b4ec3a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8785 sha256=6919446222e38a2e735507576e7204f9f54fd5e05454689d12352fc6f2ccfecc\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 watchdog pathtools\n",
            "Installing collected packages: subprocess32, docker-pycreds, configparser, sentry-sdk, smmap, gitdb, GitPython, shortuuid, pathtools, watchdog, wandb\n",
            "Successfully installed GitPython-3.1.12 configparser-5.0.1 docker-pycreds-0.4.0 gitdb-4.0.5 pathtools-0.1.2 sentry-sdk-0.19.5 shortuuid-1.0.1 smmap-3.0.4 subprocess32-3.5.4 wandb-0.10.14 watchdog-0.10.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K3-dA3lqw8U"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wm5e1lpBjTX"
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "HirEm-CYqFBg",
        "outputId": "51a2ba84-4c76-46d2-8a22-3b2b7e9dede7"
      },
      "source": [
        "# %%writefile code_counts.py\r\n",
        "\r\n",
        "import logging\r\n",
        "import os\r\n",
        "import random\r\n",
        "import time\r\n",
        "from dataclasses import dataclass, field\r\n",
        "from typing import Dict, Optional\r\n",
        "from typing import List\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "from torch.utils.data.dataset import Dataset, IterableDataset\r\n",
        "from torch.utils.tensorboard import SummaryWriter\r\n",
        "from transformers import AutoTokenizer, EvalPrediction, Trainer, HfArgumentParser, TrainingArguments, \\\r\n",
        "    AutoModelForSequenceClassification, set_seed, AutoConfig\r\n",
        "from transformers import PreTrainedTokenizer, DataCollator, PreTrainedModel\r\n",
        "import wandb\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "def load_data():\r\n",
        "    trn=pd.read_csv('/content/gdrive/My Drive/multilingual/jigsaw-toxic-comment-train.csv.zip',usecols=['toxic','comment_text'])\r\n",
        "    tst=pd.read_csv('/content/gdrive/My Drive/multilingual/test.csv.zip',usecols=['lang','content'])  \r\n",
        "    sub=pd.read_csv('/content/gdrive/My Drive/multilingual/submission.csv') \r\n",
        "    val=pd.read_csv( '/content/gdrive/My Drive/multilingual/validation.csv.zip') \r\n",
        "    return trn,tst,sub,val\r\n",
        "\r\n",
        "def get_lang(val,sub,test,lang):\r\n",
        "  output=pd.concat([val.loc[val['lang']==lang],test.loc[test['lang']==lang]]).reset_index(drop=True)\r\n",
        "  output=output.drop(['id','lang'],1)\r\n",
        "  return output\r\n",
        "\r\n",
        "def build_batches(sentences, batch_size):\r\n",
        "    batch_ordered_sentences = list()\r\n",
        "    while len(sentences) > 0:\r\n",
        "        to_take = min(batch_size, len(sentences))\r\n",
        "        select = random.randint(0, len(sentences) - to_take)\r\n",
        "        batch_ordered_sentences += sentences[select:select + to_take]\r\n",
        "        del sentences[select:select + to_take]\r\n",
        "    return batch_ordered_sentences\r\n",
        "class TextDataset(Dataset):\r\n",
        "    def __init__(self, tokenizer, pad_to_max_length, max_len,\r\n",
        "                 examples) :\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "        self.max_len = max_len\r\n",
        "        self.examples: List[Example] = examples\r\n",
        "        self.current = 0\r\n",
        "        self.pad_to_max_length = pad_to_max_length\r\n",
        "\r\n",
        "    def encode(self, ex) :\r\n",
        "        encode_dict = self.tokenizer.encode_plus(text=ex.text_a,\r\n",
        "                                                 text_pair=ex.text_b,\r\n",
        "                                                 add_special_tokens=True,\r\n",
        "                                                 max_length=self.max_len,\r\n",
        "                                                 pad_to_max_length=self.pad_to_max_length,\r\n",
        "                                                 return_token_type_ids=False,\r\n",
        "                                                 return_attention_mask=True,\r\n",
        "                                                 return_overflowing_tokens=False,\r\n",
        "                                                 return_special_tokens_mask=False,\r\n",
        "                                                 )\r\n",
        "        return Features(input_ids=encode_dict[\"input_ids\"],\r\n",
        "                        attention_mask=encode_dict[\"attention_mask\"],\r\n",
        "                        label=ex.label)\r\n",
        "\r\n",
        "    def __getitem__(self, idx) :  # Trainer doesn't support IterableDataset (define sampler)\r\n",
        "        if self.current == len(self.examples):\r\n",
        "            self.current = 0\r\n",
        "        ex = self.examples[self.current]\r\n",
        "        self.current += 1\r\n",
        "        return self.encode(ex=ex)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.examples)\r\n",
        "def load_transformers_model(pretrained_model_name_or_path,\r\n",
        "                            use_cuda,\r\n",
        "                            mixed_precision):\r\n",
        "    config = AutoConfig.from_pretrained(pretrained_model_name_or_path=pretrained_model_name_or_path,\r\n",
        "                                        num_labels=3)\r\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\r\n",
        "        pretrained_model_name_or_path=pretrained_model_name_or_path,\r\n",
        "        config=config)\r\n",
        "    if use_cuda and torch.cuda.is_available():\r\n",
        "        device = torch.device('cuda')\r\n",
        "        model.to(device)\r\n",
        "\r\n",
        "    if mixed_precision:\r\n",
        "        try:\r\n",
        "            from apex import amp\r\n",
        "            model = amp.initialize(model, opt_level='O1')\r\n",
        "        except ImportError:\r\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\r\n",
        "    return model\r\n",
        "class MyTrainer(Trainer):\r\n",
        "    def _setup_wandb(self):\r\n",
        "        wandb.init(project=\"speed_training\",\r\n",
        "                   config=vars(self.args),\r\n",
        "                   name=self.args.output_dir)\r\n",
        "        wandb.watch(self.model, log=\"gradients\", log_freq=self.args.logging_steps)\r\n",
        "\r\n",
        "\r\n",
        "def compute_metrics(p: EvalPrediction)  :\r\n",
        "        preds = np.argmax(p.predictions, axis=1)\r\n",
        "        return {\"acc\": (preds == p.label_ids).mean()}\r\n",
        "\r\n",
        "\r\n",
        "@dataclass\r\n",
        "class ModelParameters:\r\n",
        "    max_seq_len: Optional[int] = field(\r\n",
        "        default=None,\r\n",
        "        metadata={\"help\": \"max seq len\"},\r\n",
        "    )\r\n",
        "    dynamic_padding: bool = field(\r\n",
        "        default=False,\r\n",
        "        metadata={\"help\": \"limit pad size at batch level\"},\r\n",
        "    )\r\n",
        "    smart_batching: bool = field(\r\n",
        "        default=False,\r\n",
        "        metadata={\"help\": \"build batch of similar sizes\"},\r\n",
        "    )\r\n",
        "    dynamic_batch_size: bool = field(\r\n",
        "        default=False,\r\n",
        "        metadata={\"help\": \"build batch of similar sizes\"},\r\n",
        "    )\r\n",
        "class SmartCollator(DataCollator):\r\n",
        "    pad_token_id: int\r\n",
        "\r\n",
        "    def collate_batch(self, batch: List[Features]) -> Dict[str, torch.Tensor]:\r\n",
        "        batch_inputs = list()\r\n",
        "        batch_attention_masks = list()\r\n",
        "        labels = list()\r\n",
        "        max_size = max([len(ex.input_ids) for ex in batch])\r\n",
        "        for item in batch:\r\n",
        "            batch_inputs += [pad_seq(item.input_ids, max_size, self.pad_token_id)]\r\n",
        "            batch_attention_masks += [pad_seq(item.attention_mask, max_size, 0)]\r\n",
        "            labels.append(item.label)\r\n",
        "\r\n",
        "        return {\"input_ids\": torch.tensor(batch_inputs, dtype=torch.long),\r\n",
        "                \"attention_mask\": torch.tensor(batch_attention_masks, dtype=torch.long),\r\n",
        "                \"labels\": torch.tensor(labels, dtype=torch.long)\r\n",
        "                }\r\n",
        "\r\n",
        "if __name__ == \"__main__\":\r\n",
        "    # parser = HfArgumentParser((TrainingArguments, ModelParameters))\r\n",
        "    # training_args, model_args = parser.parse_args_into_dataclasses()  # type: (TrainingArguments, ModelParameters)\r\n",
        "\r\n",
        " \r\n",
        "    train,test,sub,val=load_data()\r\n",
        "    test.columns=['comment_text','lang']\r\n",
        "    test['toxic']=sub['toxic']\r\n",
        "\r\n",
        "    french=get_lang(val,sub,test,'fr')\r\n",
        "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"camembert-base\")\r\n",
        "    max_sequence_len=192\r\n",
        "\r\n",
        "    train_batches = build_batches(sentences=list(french.comment_text.values), batch_size=16*8)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    train_set = TextDataset(tokenizer=tokenizer,\r\n",
        "                            max_len=max_sequence_len,\r\n",
        "                            examples=train_batches,\r\n",
        "                            pad_to_max_length=True)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    model = load_transformers_model(pretrained_model_name_or_path=\"camembert-base\",\r\n",
        "                                    use_cuda=True,\r\n",
        "                                    mixed_precision=False)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    trainer = MyTrainer(\r\n",
        "        model=model,\r\n",
        "        # args=training_args,\r\n",
        "        train_dataset=train_set,\r\n",
        "        # data_collator=IdentityCollator(pad_token_id=tokenizer.pad_token_id),\r\n",
        "        data_collator=SmartCollator(pad_token_id=tokenizer.pad_token_id),\r\n",
        "        tb_writer=SummaryWriter(log_dir='logs', flush_secs=10),\r\n",
        "        eval_dataset=valid_set,\r\n",
        "        compute_metrics=compute_metrics,\r\n",
        "      )\r\n",
        "    \r\n",
        "\r\n",
        "    start_time = time.time()\r\n",
        "    trainer.train()\r\n",
        "    wandb.config.update(model_args)\r\n",
        "    wandb.config.update(training_args)\r\n",
        "    wandb.log({\"training time\": int((time.time() - start_time) / 60)})\r\n",
        "    trainer.save_model()\r\n",
        "    trainer.evaluate()\r\n",
        "    logging.info(\"*** Evaluate ***\")\r\n",
        "    result = trainer.evaluate()\r\n",
        "    wandb.log(result)\r\n",
        "\r\n",
        "    output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\r\n",
        "    with open(output_eval_file, \"w\") as writer:\r\n",
        "        logging.info(\"***** Eval results *****\")\r\n",
        "        for key, value in result.items():\r\n",
        "            logging.info(\"  %s = %s\", key, value)\r\n",
        "            writer.write(\"%s = %s\\n\" % (key, value))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-2f1544aaeb09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m# data_collator=IdentityCollator(pad_token_id=tokenizer.pad_token_id),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mdata_collator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSmartCollator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0mtb_writer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SmartCollator' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvCCzjxVqWPo",
        "outputId": "0af0dfcd-f05b-424b-90bc-b6a0826d6534"
      },
      "source": [
        "!python code_counts.py \\\r\n",
        " \t\t\t--output_dir ./models/speed_camembert_max_len_128_fp16_dynamic_padding_smart_batching_batch_64_seed_321 \\\r\n",
        " \t\t\t--overwrite_output_dir \\\r\n",
        " \t\t\t--save_steps 0 \\\r\n",
        " \t\t\t--seed 321 \\\r\n",
        " \t\t\t--num_train_epochs 1 \\\r\n",
        " \t\t\t--learning_rate 5e-5 \\\r\n",
        " \t\t\t--per_gpu_train_batch_size 64 \\\r\n",
        " \t\t\t--gradient_accumulation_steps 1 \\\r\n",
        " \t\t\t--per_gpu_eval_batch_size 64 \\\r\n",
        " \t\t\t--max_seq_len 128 \\\r\n",
        " \t\t\t--dynamic_padding \\\r\n",
        " \t\t\t--smart_batching \\\r\n",
        " \t\t\t--fp16 \\\r\n",
        " \t\t\t--evaluate_during_training ; \\\r\n",
        "\t)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: -c: line 0: syntax error near unexpected token `)'\n",
            "/bin/bash: -c: line 0: `python code_counts.py  \t\t\t--output_dir ./models/speed_camembert_max_len_128_fp16_dynamic_padding_smart_batching_batch_64_seed_321  \t\t\t--overwrite_output_dir  \t\t\t--save_steps 0  \t\t\t--seed 321  \t\t\t--num_train_epochs 1  \t\t\t--learning_rate 5e-5  \t\t\t--per_gpu_train_batch_size 64  \t\t\t--gradient_accumulation_steps 1  \t\t\t--per_gpu_eval_batch_size 64  \t\t\t--max_seq_len 128  \t\t\t--dynamic_padding  \t\t\t--smart_batching  \t\t\t--fp16  \t\t\t--evaluate_during_training ; \t)'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "W29YTbzwp2K7",
        "outputId": "db6adfe5-c4e0-4bb4-9b2b-ad244712a1c0"
      },
      "source": [
        "val.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>lang</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Este usuario ni siquiera llega al rango de    ...</td>\n",
              "      <td>es</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Il testo di questa voce pare esser scopiazzato...</td>\n",
              "      <td>it</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Vale. Sólo expongo mi pasado. Todo tiempo pasa...</td>\n",
              "      <td>es</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Bu maddenin alt başlığı olarak  uluslararası i...</td>\n",
              "      <td>tr</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Belçika nın şehirlerinin yanında ilçe ve belde...</td>\n",
              "      <td>tr</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                       comment_text lang  toxic\n",
              "0   0  Este usuario ni siquiera llega al rango de    ...   es      0\n",
              "1   1  Il testo di questa voce pare esser scopiazzato...   it      0\n",
              "2   2  Vale. Sólo expongo mi pasado. Todo tiempo pasa...   es      1\n",
              "3   3  Bu maddenin alt başlığı olarak  uluslararası i...   tr      0\n",
              "4   4  Belçika nın şehirlerinin yanında ilçe ve belde...   tr      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Zr-pucdBpzda",
        "outputId": "145e94f3-1378-4dc9-a47d-df1b9876b5a7"
      },
      "source": [
        "sub.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>toxic</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000081</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.003086</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.308321</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000055</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000143</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      toxic  id\n",
              "0  0.000081   0\n",
              "1  0.003086   1\n",
              "2  0.308321   2\n",
              "3  0.000055   3\n",
              "4  0.000143   4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "nSgl1VtBpyGD",
        "outputId": "647e3b0f-fe9a-4be1-8395-edecc6197a08"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>lang</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Doctor Who adlı viki başlığına 12. doctor olar...</td>\n",
              "      <td>tr</td>\n",
              "      <td>0.000081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Вполне возможно, но я пока не вижу необходимо...</td>\n",
              "      <td>ru</td>\n",
              "      <td>0.003086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Quindi tu sei uno di quelli   conservativi  , ...</td>\n",
              "      <td>it</td>\n",
              "      <td>0.308321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Malesef gerçekleştirilmedi ancak şöyle bir şey...</td>\n",
              "      <td>tr</td>\n",
              "      <td>0.000055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>:Resim:Seldabagcan.jpg resminde kaynak sorunu ...</td>\n",
              "      <td>tr</td>\n",
              "      <td>0.000143</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        comment_text lang     toxic\n",
              "0  Doctor Who adlı viki başlığına 12. doctor olar...   tr  0.000081\n",
              "1   Вполне возможно, но я пока не вижу необходимо...   ru  0.003086\n",
              "2  Quindi tu sei uno di quelli   conservativi  , ...   it  0.308321\n",
              "3  Malesef gerçekleştirilmedi ancak şöyle bir şey...   tr  0.000055\n",
              "4  :Resim:Seldabagcan.jpg resminde kaynak sorunu ...   tr  0.000143"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XxHEiDCgpoct",
        "outputId": "7ed074bc-e7bf-424d-b12d-f7f8930ca006"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        comment_text  toxic\n",
              "0  Explanation\\nWhy the edits made under my usern...      0\n",
              "1  D'aww! He matches this background colour I'm s...      0\n",
              "2  Hey man, I'm really not trying to edit war. It...      0\n",
              "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
              "4  You, sir, are my hero. Any chance you remember...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtpMbQGeCBT-"
      },
      "source": [
        "import os\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.layers import Dense, Input\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\r\n",
        "import transformers\r\n",
        "from transformers import TFAutoModel, AutoTokenizer\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\r\n",
        "from tensorflow.keras.layers import *\r\n",
        "from fastai.text import *\r\n",
        "import multifit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "jcmqB8KgCHK3",
        "outputId": "e4d6ee56-bff3-4244-cbfd-b3f0ea21d37f"
      },
      "source": [
        "es=test.loc[test['lang']=='es'].reset_index(drop=True)\r\n",
        "es.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>lang</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>el skate es unos de los deportes favoritos de ...</td>\n",
              "      <td>es</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Me doy la bienvenida. A este usuari le gusta c...</td>\n",
              "      <td>es</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ES NOTABLEMENTE TENDENCIOSO, NO SE HABLA DE CU...</td>\n",
              "      <td>es</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>El Jardín de infantes Nº938, fundado en 1989, ...</td>\n",
              "      <td>es</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Daré explicaciones y/o aclaraciones a cualquie...</td>\n",
              "      <td>es</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content lang  toxic\n",
              "0  el skate es unos de los deportes favoritos de ...   es    0.0\n",
              "1  Me doy la bienvenida. A este usuari le gusta c...   es    0.0\n",
              "2  ES NOTABLEMENTE TENDENCIOSO, NO SE HABLA DE CU...   es    0.0\n",
              "3  El Jardín de infantes Nº938, fundado en 1989, ...   es    0.0\n",
              "4  Daré explicaciones y/o aclaraciones a cualquie...   es    0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itanvsgaE4L-"
      },
      "source": [
        "def set_seed(seed):\r\n",
        "    if seed is not None:\r\n",
        "        torch.manual_seed(seed)\r\n",
        "        torch.backends.cudnn.deterministic = True\r\n",
        "        torch.backends.cudnn.benchmark = False\r\n",
        "        np.random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "s8DbvZBPCZ_F",
        "outputId": "d3cdff7f-aa70-447c-c9f5-d20a7508c7d6"
      },
      "source": [
        "exp = multifit.from_pretrained(\"es_multifit_paper_version\")\r\n",
        "tokenizer = exp.pretrain_lm.tokenizer\r\n",
        "fa_config =  tokenizer.get_fastai_config(add_open_file_processor=True)\r\n",
        "data_lm = (TextList.from_df(es, **fa_config)\r\n",
        "            .split_by_rand_pct(0.1)\r\n",
        "            .label_for_lm()           \r\n",
        "            .databunch(bs=32))\r\n",
        "exp.finetune_lm.arch.lang = 'es'\r\n",
        "set_seed(42)\r\n",
        "learn = exp.finetune_lm.get_learner(data_lm)  \r\n",
        "    # learn is a preconfigured fastai learner with a pretrained model loaded\r\n",
        "data_lm.show_batch()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/n-waves/multifit-models/releases/download/es_multifit_paper_version/es_multifit_paper_version.tgz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Training args:  {'drop_mult': 0.3, 'true_wd': False, 'wd': 1e-07, 'pretrained': False, 'clip': 0.12} config:  {'emb_sz': 400, 'n_hid': 1550, 'n_layers': 4, 'pad_token': 1, 'qrnn': True, 'bidir': False, 'output_p': 0.25, 'hidden_p': 0.1, 'input_p': 0.2, 'embed_p': 0.02, 'weight_p': 0.15, 'tie_weights': True, 'out_bias': True}\n",
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "Setting LM training seed seed to 0\n",
            "Loading pretrained weights:  [PosixPath('/root/.fastai/models/es_multifit_paper_version/lm_best'), PosixPath('/root/.fastai/models/es_multifit_paper_version/itos')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/fastai/text/data.py:339: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero()\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  idx_min = (t != self.pad_idx).nonzero().min()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>idx</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>▁, ▁ xxup ▁no ▁ xxup ▁se ▁ xxup ▁habla ▁ xxup ▁de ▁ xxup ▁cuando ▁ xxup ▁la ▁ xxup ▁tu pac ▁ xxup ▁amenaza ba ▁y ▁ xxup ▁fue ▁ xxup ▁denuncia do ▁ xxup ▁por ▁3 ▁ xxup ▁fiscal es ▁ xxup ▁en ▁ xxup ▁febrero ▁, ▁ xxup ▁ni ▁ xxup ▁de ▁ xxup ▁cama ras ▁ xxup ▁oculta s ▁, ▁ xxup ▁ni ▁ xxup</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>▁para ▁que ▁lo ▁cu bri eran ▁en ▁sus ▁fe cho ría s ▁y ▁ahora ▁dice ▁su ▁escrito ▁que ▁es ▁un ▁héroe ▁nacional ▁. ▁ xxmaj ▁band ido ▁de ▁7 ▁su ela s ▁ xxbos ▁ xxfld ▁1 ▁ xxmaj ▁ cre o ▁que ▁después ▁de ▁mi ▁posterior ▁ edi t ▁todo ▁quedó ▁bien ▁. ▁ xxmaj ▁gracias ▁y ▁salud os ▁. ▁ xxbos ▁ xxfld ▁1 ▁ xxmaj ▁se ▁ha</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>os ▁. ▁ xxmaj ▁pues ▁bueno ▁, ▁inicia ndo ▁con ▁el ▁tema ▁, ▁muy ▁di fici l mente ▁ cre o ▁que ▁no ▁se ▁haya ▁dado ▁cuenta ▁del ▁tiempo ▁de ▁su ▁bloqueo ▁puesto ▁que ▁siempre ▁, ▁cuando ▁inicia s ▁sesión ▁e ▁intenta s ▁ edi tar ▁, ▁el ▁sistema ▁ te ▁ avi sa ▁de ▁cu án to ▁tiempo ▁es ▁el ▁bloqueo ▁y ▁por ▁qué ▁razones ▁. ▁ xxmaj ▁en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>xxmaj ▁es ▁una ▁traducción ▁de ▁http ▁: ▁/ ▁/ ▁www . ri bb on s o ft . com ▁/ ▁ q ca d . html . ▁ xxmaj ▁no ▁esto y ▁seguro ▁que ▁está ▁bajo ▁ xxup ▁ g f d l ▁/ ▁ xxup ▁ g pl ▁. ▁ xxmaj ▁porque ▁la ▁empresa ▁hace ▁también ▁software ▁que ▁se ▁bloque en ▁después ▁de ▁10 ▁minutos ▁para ▁que ▁la ▁gente</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>▁ xxmaj ▁quizá ▁habría ▁que ▁trasladar lo ▁de ▁momento ▁a ▁ xxmaj ▁intento ▁de ▁ xxmaj ▁golpe ▁de ▁ xxmaj ▁estado ▁en ▁ xxmaj ▁turquía ▁de ▁2016 ▁, ▁pero ▁habría ▁que ▁borra r ▁la ▁re di rec ción ▁que ▁he ▁creado ▁. ▁18 p x ▁ xxmaj ▁me ▁parece ▁bien ▁/ ▁ xxmaj ▁salud os ▁18 p x ▁ xxbos ▁ xxfld ▁1 ▁ xxmaj ▁hijo ▁de ▁pu ta ▁,</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDNDj3MNB9mA"
      },
      "source": [
        "# learn.freeze_to(-1)\r\n",
        "# learn.fit_one_cycle(1, 1e-4 * 10, moms=(0.8, 0.7))\r\n",
        "# learn.unfreeze()\r\n",
        "# learn.fit_one_cycle(10, 1e-4, moms=(0.8, 0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRydnRKIMWKC"
      },
      "source": [
        "path='/content/gdrive/My Drive/multilingual toxic/es/es1'\r\n",
        "self=exp.finetune_lm\r\n",
        "CLS_BEST = path+'/cls_best1'\r\n",
        "LM_BEST = path+\"/lm_best1\"\r\n",
        "ENC_BEST = path+\"/enc_best1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qj4uCxyr3kXn",
        "outputId": "be11c7e9-9865-4466-c23d-4f5ea7486ac1"
      },
      "source": [
        "path='/content/gdrive/My Drive/multilingual toxic/es/es1'\r\n",
        "self=exp.finetune_lm\r\n",
        "CLS_BEST = path+'/cls_best'\r\n",
        "LM_BEST = path+\"/lm_best\"\r\n",
        "ENC_BEST = path+\"/enc_best\"\r\n",
        "\r\n",
        "\r\n",
        "experiment_path=Path(path)\r\n",
        "self.experiment_path=experiment_path\r\n",
        "print(\"Experiment\", experiment_path)\r\n",
        "\r\n",
        "#save tokenizer\r\n",
        "tokenizer.save(self.experiment_path, learn=learn)\r\n",
        "learn.to_fp32()\r\n",
        "#save encoder weights to (ENC_BEST,learn.path,learn.model_dir)\r\n",
        "learn.save_encoder(ENC_BEST)\r\n",
        "#save model structure to (LM_BEST, learn.path,learn.model_dir,'.pth')\r\n",
        "learn.save(LM_BEST, with_opt=False)\r\n",
        "# learn.destroy()\r\n",
        "# saving 'seed', 'name', 'arch', 'experiment_path', 'dataset_path', 'num_epochs', 'bs', 'bptt', 'drop_mult', 'dropout_values', 'label_smoothing_eps', 'label_smoothing_eps_norm_by_classes', 'use_adam_08', 'true_wd', 'wd', 'clip', 'fp16', 'lr', 'base' \r\n",
        "# to /content/gdrive/My Drive/multilingual toxic/es/es1/finetuning.json\r\n",
        "# save_paramters(self)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experiment /content/gdrive/My Drive/multilingual toxic/es/es1\n",
            "Copy sp model from /root/.fastai/models/es_multifit_paper_version to /content/gdrive/My Drive/multilingual toxic/es/es1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "0uTQR6_sRFsP",
        "outputId": "e0d2ef85-ffb3-4905-f3df-096b1080db6b"
      },
      "source": [
        "self=exp.classifier\r\n",
        "es['toxic']=es['toxic'].astype(np.long)\r\n",
        "from pathlib import Path\r\n",
        "data_clas = (TextList.from_df(es,cols=['content','toxic'], **fa_config)\r\n",
        "            .split_by_rand_pct(0.1)\r\n",
        "            .label_from_df('toxic')           \r\n",
        "            .databunch(bs=32))\r\n",
        "l1rn = exp.classifier.get_learner(data_clas)  \r\n",
        "# learn is a preconfigured fastai learner with a pretrained model loaded\r\n",
        "data_clas.show_batch()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using Label smoothing with eps =  0.05\n",
            "Setting Classifier weights seed seed to 0\n",
            "Training args:  {'drop_mult': 0.5, 'wd': 0.01, 'pretrained': False, 'bptt': 70, 'loss_func': FlattenedLoss of LabelSmoothingCrossEntropy(), 'clip': 0.12, 'silent': False} config:  {'emb_sz': 400, 'n_hid': 1550, 'n_layers': 4, 'pad_token': 1, 'qrnn': True, 'bidir': False, 'output_p': 0.25, 'hidden_p': 0.1, 'input_p': 0.2, 'embed_p': 0.02, 'weight_p': 0.15}\n",
            "Loading pretrained model /content/gdrive/My Drive/multilingual toxic/es/es1/enc_best\n",
            "Setting Classifier training seed seed to 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>▁ xxbos ▁ xxfld ▁1 ▁http ▁: ▁/ ▁/ ▁www . the - af c . com ▁/ ▁en ▁/ ▁e vent - in form a tion ▁/ ▁ af c - asi an - cu p - ma tch - off icia l s ▁http ▁: ▁/ ▁/ ▁www . the - af c . com ▁/ ▁en ▁/ ▁compone nt ▁/ ▁ jo om lea gue ▁/ ▁</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>▁ xxbos ▁ xxfld ▁1 ▁ xxup ▁san ▁ xxup ▁ ba udi lio ▁ n ▁ xxrep ▁ 44 ▁o ▁ xxup ▁san ▁ xxup ▁ ba udi lio ▁ n ▁ xxrep ▁ 44 ▁o ▁ xxup ▁san ▁ xxup ▁ ba udi lio ▁ n ▁ xxrep ▁ 44 ▁o ▁ xxup ▁san ▁ xxup ▁ ba udi lio ▁ n ▁ xxrep ▁ 44 ▁o ▁</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>▁ xxbos ▁ xxfld ▁1 ▁ xxup ▁señor ▁ xxup ▁ iva n ▁ xxup ▁ ce pe da ▁, ▁ xxup ▁duro ▁ xxup ▁ cri tico ▁ xxup ▁del ▁ xxup ▁mejor ▁ xxup ▁presidente ▁ xxup ▁que ▁ xxup ▁he mos ▁ xxup ▁tenido s ▁ xxup ▁los ▁ xxup ▁colombiano s ▁ xxup ▁ vic tima ▁ xxup ▁de ▁ xxup ▁las ▁ xxup ▁a tro</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>▁ xxbos ▁ xxfld ▁1 ▁no ▁mas ▁de s on rra s ▁chica s ▁... ▁re be nte mos ▁una ▁re pres a ▁, ▁como ▁, ▁ ra pel ▁, ▁mau le ▁. . ▁ oh ▁haciendo la ▁ xxup ▁esta lla r ▁... ▁ xxup ▁ca os ▁ xxup ▁total ▁ xxup ▁en ▁ xxup ▁chile ▁... ▁ xxup ▁honra ▁ xxup ▁ ah ▁ xxup ▁nivel ▁ xxup</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>▁ xxbos ▁ xxfld ▁1 ▁ xxup ▁cada ▁ xxup ▁ dia ▁ xxup ▁es ▁ xxup ▁mas ▁ xxup ▁evidente ▁ xxup ▁que ▁ xxup ▁el ▁ xxup ▁pueblo ▁ xxup ▁colombiano ▁ xxup ▁rechaza ▁ xxup ▁cada ▁ xxup ▁vez ▁ xxup ▁mas ▁ xxup ▁los ▁ xxup ▁actos ▁, ▁ xxup ▁poli tica s ▁e ▁ xxup ▁ ide ologi a ▁ xxup ▁po dri da ▁</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAHNpCK0Wj6N"
      },
      "source": [
        "l1rn.unfreeze()\r\n",
        "def _fit_schedule_1cycle(num_epochs, learn):\r\n",
        "        learn.unfreeze()\r\n",
        "        learn.fit_one_cycle(num_epochs, slice(1e-2 / (2.6 ** 4), 2e-2), moms=(0.8, 0.7))\r\n",
        "\r\n",
        "def _fit_schedule_layered(num_epochs, learn):\r\n",
        "        learn.freeze_to(-1)\r\n",
        "        learn.fit_one_cycle(1, 2e-2, moms=(0.8, 0.7))\r\n",
        "        if num_epochs > 1:\r\n",
        "            learn.freeze_to(-2)\r\n",
        "            learn.fit_one_cycle(1, slice(1e-2 / (2.6 ** 4), 1e-2), moms=(0.8, 0.7))\r\n",
        "            learn.freeze_to(-3)\r\n",
        "            learn.fit_one_cycle(1, slice(5e-3 / (2.6 ** 4), 5e-3), moms=(0.8, 0.7))\r\n",
        "            learn.unfreeze()\r\n",
        "        if num_epochs > 5:\r\n",
        "            learn.fit_one_cycle(num_epochs - 4, slice(1e-3 / (2.6 ** 4), 1e-3), moms=(0.8, 0.7))\r\n",
        "\r\n",
        "def _fit_schedule_2cycle(num_epochs, learn):\r\n",
        "        learn.freeze_to(-1)\r\n",
        "        learn.fit_one_cycle(1, 2e-2, moms=(0.8, 0.7))\r\n",
        "        learn.unfreeze()\r\n",
        "        if num_epochs > 1:\r\n",
        "            learn.fit_one_cycle(num_epochs - 1, slice(1e-2 / (2.6 ** 4), 1e-2), moms=(0.8, 0.7))\r\n",
        "\r\n",
        "def _fit_schedule_reverse_2cycle(num_epochs, learn):\r\n",
        "        learn.unfreeze()\r\n",
        "        for g in learn.layer_groups[-1:]:\r\n",
        "            for l in g:\r\n",
        "                if not learn.train_bn or not isinstance(l, bn_types): requires_grad(l, False)\r\n",
        "        learn.create_opt(defaults.lr)\r\n",
        "        learn.fit_one_cycle(num_epochs, slice(1e-2 / (2.6 ** 4), 2e-2), moms=(0.8, 0.7))\r\n",
        "        learn.unfreeze()\r\n",
        "        learn.fit_one_cycle(num_epochs, slice(1e-3 / (2.6 ** 4), 2e-3), moms=(0.8, 0.7))\r\n",
        "\r\n",
        "def _fit_schedule_false_wd(self, learn):\r\n",
        "        learn.true_wd = False\r\n",
        "        learn.fit_one_cycle(1, 5e-2, moms=(0.8, 0.7), wd=1e-7)\r\n",
        "        if num_epochs > 1:\r\n",
        "            learn.freeze_to(-2)\r\n",
        "            learn.fit_one_cycle(1, slice(5e-2 / (2.6 ** 4), 5e-2), moms=(0.8, 0.7), wd=1e-7)\r\n",
        "            learn.freeze_to(-3)\r\n",
        "            learn.fit_one_cycle(1, slice(5e-4 / (2.6 ** 4), 5e-4), moms=(0.8, 0.7), wd=1e-7)\r\n",
        "            learn.unfreeze()\r\n",
        "            if num_epochs > 5:\r\n",
        "                learn.fit_one_cycle(num_epochs - 4, slice(1e-2 / (2.6 ** 4), 1e-2), moms=(0.8, 0.7), wd=1e-7)\r\n",
        "\r\n",
        "# _fit_schedule_1cycle(10,l1rn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "VVVyTyAoWn3W",
        "outputId": "f129360b-3640-43cb-8e2e-c41abb35e4ce"
      },
      "source": [
        "self.experiment_path = l1rn.path / l1rn.model_dir\r\n",
        "tokenizer.save(self.experiment_path, learn=l1rn)\r\n",
        "l1rn.to_fp32()\r\n",
        "l1rn.save(CLS_BEST, with_opt=False)\r\n",
        "print(\"Classifier model saved to\", self.experiment_path)\r\n",
        "self.save_paramters()\r\n",
        "        # learn.destroy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copy sp model from /root/.fastai/models/es_multifit_paper_version to multifit_paper_version\n",
            "Classifier model saved to multifit_paper_version\n",
            "Saving dump to multifit_paper_version/classifier.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'{\\n  \"seed\": 0,\\n  \"name\": \"multifit_paper_version\",\\n  \"arch\": {\\n    \"tokenizer_type\": \"sp\",\\n    \"max_vocab\": 15000,\\n    \"lang\": \"es\",\\n    \"emb_sz\": 400,\\n    \"n_hid\": 1550,\\n    \"n_layers\": 4,\\n    \"qrnn\": true\\n  },\\n  \"experiment_path\": \"multifit_paper_version\",\\n  \"dataset_path\": null,\\n  \"bs\": 18,\\n  \"num_epochs\": 8,\\n  \"drop_mult\": 0.5,\\n  \"dropout_values\": {\\n    \"output_p\": 0.25,\\n    \"hidden_p\": 0.1,\\n    \"input_p\": 0.2,\\n    \"embed_p\": 0.02,\\n    \"weight_p\": 0.15\\n  },\\n  \"wd\": 0.01,\\n  \"clip\": 0.12,\\n  \"label_smoothing_eps\": 0.1,\\n  \"label_smoothing_eps_norm_by_classes\": true,\\n  \"weighted_cross_entropy\": null,\\n  \"early_stopping\": null,\\n  \"fit_schedule\": \"1cycle\",\\n  \"random_init\": false,\\n  \"bptt\": 70,\\n  \"fp16\": false,\\n  \"base\": \"/content/gdrive/My Drive/multilingual toxic/es/es1\"\\n}'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN1NyDNKZuYw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}